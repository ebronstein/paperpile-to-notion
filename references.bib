@ARTICLE{Jiang-MInference-1Attention-2024,
  title        = {{MInference} 1.0: Accelerating pre-filling for long-context
                  {LLMs} via dynamic sparse attention},
  author       = {Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and
                  Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua
                  and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and Yang,
                  Yuqing and Qiu, Lili},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-07-02},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {The computational challenges of Large Language Model (LLM)
                  inference remain a significant barrier to their widespread
                  deployment, especially as prompt lengths continue to increase.
                  Due to the quadratic complexity of the attention computation,
                  it takes 30 minutes for an 8B LLM to process a prompt of 1M
                  tokens (i.e., the pre-filling stage) on a single A100 GPU.
                  Existing methods for speeding up prefilling often fail to
                  maintain acceptable accuracy or efficiency when applied to
                  long-context LLMs. To address this gap, we introduce
                  MInference (Milliontokens Inference), a sparse calculation
                  method designed to accelerate pre-filling of long-sequence
                  processing. Specifically, we identify three unique patterns in
                  long-context attention matrices-the A-shape, Vertical-Slash,
                  and Block-Sparsethat can be leveraged for efficient sparse
                  computation on GPUs. We determine the optimal pattern for each
                  attention head offline and dynamically build sparse indices
                  based on the assigned pattern during inference. With the
                  pattern and sparse indices, we perform efficient sparse
                  attention calculations via our optimized GPU kernels to
                  significantly reduce the latency in the pre-filling stage of
                  long-context LLMs. Our proposed technique can be directly
                  applied to existing LLMs without any modifications to the
                  pre-training setup or additional fine-tuning. By evaluating on
                  a wide range of downstream tasks, including InfiniteBench,
                  RULER, PG-19, and Needle In A Haystack, and models including
                  LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we
                  demonstrate that MInference effectively reduces inference
                  latency by up to 10x for pre-filling on an A100, while
                  maintaining accuracy. Our code is available at
                  https://aka.ms/MInference.},
  urldate      = {2024-11-23}
}

@ARTICLE{Kuzmin-FP8-QuantizationExponent-2022,
  title        = {{FP8} quantization: The power of the exponent},
  author       = {Kuzmin, Andrey and Van Baalen, Mart and Ren, Yuwei and Nagel,
                  Markus and Peters, Jorn and Blankevoort, Tijmen},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-08-19},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {When quantizing neural networks for efficient inference,
                  low-bit integers are the go-to format for efficiency. However,
                  low-bit floating point numbers have an extra degree of
                  freedom, assigning some bits to work on an exponential scale
                  instead. This paper in-depth investigates this benefit of the
                  floating point format for neural network inference. We detail
                  the choices that can be made for the FP8 format, including the
                  important choice of the number of bits for the mantissa and
                  exponent, and show analytically in which settings these
                  choices give better performance. Then we show how these
                  findings translate to real networks, provide an efficient
                  implementation for FP8 simulation, and a new algorithm that
                  enables the learning of both the scale parameters and the
                  number of exponent bits in the FP8 format. Our chief
                  conclusion is that when doing post-training quantization for a
                  wide range of networks, the FP8 format is better than INT8 in
                  terms of accuracy, and the choice of the number of exponent
                  bits is driven by the severity of outliers in the network. We
                  also conduct experiments with quantization-aware training
                  where the difference in formats disappears as the network is
                  trained to reduce the effect of outliers.},
  urldate      = {2024-10-05}
}

@ARTICLE{Lin-AWQ-Activation-awareAcceleration-2023,
  title        = {{AWQ}: Activation-aware Weight Quantization for {LLM}
                  compression and acceleration},
  author       = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang
                  and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and
                  Dang, Xingyu and Gan, Chuang and Han, Song},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-06-01},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Large language models (LLMs) have transformed numerous AI
                  applications. On-device LLM is becoming increasingly
                  important: running LLMs locally on edge devices can reduce the
                  cloud computing cost and protect users' privacy. However, the
                  astronomical model size and the limited hardware resource pose
                  significant deployment challenges. We propose Activation-aware
                  Weight Quantization (AWQ), a hardware-friendly approach for
                  LLM low-bit weight-only quantization. AWQ finds that not all
                  weights in an LLM are equally important. Protecting only 1\%
                  salient weights can greatly reduce quantization error. To
                  identify salient weight channels, we should refer to the
                  activation distribution, not weights. To avoid the
                  hardware-inefficient mix-precision quantization, we
                  mathematically derive that scaling up the salient channels can
                  reduce the quantization error. AWQ employs an equivalent
                  transformation to scale the salient weight channels to protect
                  them. The scale is determined by collecting the activation
                  statistics offline. AWQ does not rely on any backpropagation
                  or reconstruction, so it generalizes to different domains and
                  modalities without overfitting the calibration set. AWQ
                  outperforms existing work on various language modeling and
                  domain-specific benchmarks (coding and math). Thanks to better
                  generalization, it achieves excellent quantization performance
                  for instruction-tuned LMs and, for the first time, multi-modal
                  LMs. Alongside AWQ, we implement TinyChat, an efficient and
                  flexible inference framework tailored for 4-bit on-device
                  LLM/VLMs. With kernel fusion and platform-aware weight
                  packing, TinyChat offers more than 3x speedup over the
                  Huggingface FP16 implementation on both desktop and mobile
                  GPUs. It also democratizes the deployment of the 70B Llama-2
                  model on mobile GPUs.},
  urldate      = {2024-10-05}
}

@ARTICLE{Kwon-Efficient-MemoryPagedAttention-2023,
  title        = {Efficient memory management for large language model serving
                  with {PagedAttention}},
  author       = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng,
                  Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph
                  E and Zhang, Hao and Stoica, Ion},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-09-12},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {High throughput serving of large language models (LLMs)
                  requires batching sufficiently many requests at a time.
                  However, existing systems struggle because the key-value cache
                  (KV cache) memory for each request is huge and grows and
                  shrinks dynamically. When managed inefficiently, this memory
                  can be significantly wasted by fragmentation and redundant
                  duplication, limiting the batch size. To address this problem,
                  we propose PagedAttention, an attention algorithm inspired by
                  the classical virtual memory and paging techniques in
                  operating systems. On top of it, we build vLLM, an LLM serving
                  system that achieves (1) near-zero waste in KV cache memory
                  and (2) flexible sharing of KV cache within and across
                  requests to further reduce memory usage. Our evaluations show
                  that vLLM improves the throughput of popular LLMs by
                  2-4$\times$ with the same level of latency compared to the
                  state-of-the-art systems, such as FasterTransformer and Orca.
                  The improvement is more pronounced with longer sequences,
                  larger models, and more complex decoding algorithms. vLLM's
                  source code is publicly available at
                  https://github.com/vllm-project/vllm},
  urldate      = {2024-09-08}
}

@ARTICLE{Zheng-SGLang-EfficientPrograms-2023,
  title        = {{SGLang}: Efficient execution of structured language model
                  programs},
  author       = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun,
                  Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and
                  Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and
                  Barrett, Clark and Sheng, Ying},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-12-12},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {Large language models (LLMs) are increasingly used for complex
                  tasks that require multiple generation calls, advanced
                  prompting techniques, control flow, and structured
                  inputs/outputs. However, efficient systems are lacking for
                  programming and executing these applications. We introduce
                  SGLang, a system for efficient execution of complex language
                  model programs. SGLang consists of a frontend language and a
                  runtime. The frontend simplifies programming with primitives
                  for generation and parallelism control. The runtime
                  accelerates execution with novel optimizations like
                  RadixAttention for KV cache reuse and compressed finite state
                  machines for faster structured output decoding. Experiments
                  show that SGLang achieves up to 6.4x higher throughput
                  compared to state-of-the-art inference systems on various
                  large language and multi-modal models on tasks including agent
                  control, logical reasoning, few-shot learning benchmarks, JSON
                  decoding, retrieval-augmented generation pipelines, and
                  multi-turn chat. The code is publicly available at
                  https://github.com/sgl-project/sglang},
  urldate      = {2024-09-08}
}

@ARTICLE{Gunasekar-Textbooks-Need-2023,
  title        = {Textbooks are all you need},
  author       = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes,
                  Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth
                  and Javaheripi, Mojan and Kauffmann, Piero and de Rosa,
                  Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital
                  and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien
                  and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and
                  Li, Yuanzhi},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-06-20},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {We introduce phi-1, a new large language model for code, with
                  significantly smaller size than competing models: phi-1 is a
                  Transformer-based model with 1.3B parameters, trained for 4
                  days on 8 A100s, using a selection of ``textbook quality" data
                  from the web (6B tokens) and synthetically generated textbooks
                  and exercises with GPT-3.5 (1B tokens). Despite this small
                  scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and
                  55.5\% on MBPP. It also displays surprising emergent
                  properties compared to phi-1-base, our model before our
                  finetuning stage on a dataset of coding exercises, and
                  phi-1-small, a smaller model with 350M parameters trained with
                  the same pipeline as phi-1 that still achieves 45\% on
                  HumanEval.},
  urldate      = {2024-11-11}
}

@ARTICLE{Gemma-Team-Gemma-2Size-2024,
  title        = {Gemma 2: Improving open language models at a practical size},
  author       = {{Gemma Team} and Riviere, Morgane and Pathak, Shreya and
                  Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju,
                  Surya and Hussenot, Léonard and Mesnard, Thomas and Shahriari,
                  Bobak and Ramé, Alexandre and Ferret, Johan and Liu, Peter and
                  Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos,
                  Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy
                  and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr
                  and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and
                  Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur,
                  Behnam and Bachem, Olivier and Walton, Alanna and Severyn,
                  Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison,
                  Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and
                  Brock, Andy and Coenen, Andy and Laforge, Anthony and
                  Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo
                  and {Brandon Royal} and Chen, Charlie and Kumar, Chintu and
                  Perry, Chris and Welty, Chris and Choquette-Choo, Christopher
                  A and Sinopalnikov, Danila and Weinberger, David and
                  Vijaykumar, Dimple and Rogozińska, Dominika and Herbison,
                  Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and
                  Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and
                  Visin, Francesco and Rasskin, Gabriel and Wei, Gary and
                  Cameron, Glenn and Martins, Gus and Hashemi, Hadi and
                  Klimczak-Plucińska, Hanna and Batra, Harleen and Dhand, Harsh
                  and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and
                  Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou,
                  Jin Peng and Carrasqueira, Joana and Iljazi, Joana and Becker,
                  Jocelyn and Fernandez, Joe and van Amersfoort, Joost and
                  Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji,
                  Ju-Yeong and Mohamed, Kareem and Badola, Kartikeya and Black,
                  Kat and Millican, Katie and McDonell, Keelin and Nguyen,
                  Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund,
                  Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann,
                  Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio
                  Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins,
                  Luciano and Reid, Machel and Singh, Manvinder and Iverson,
                  Mark and Görner, Martin and Velloso, Mat and Wirth, Mateo and
                  Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson,
                  Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan,
                  Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and
                  Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and
                  Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and
                  Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and
                  Barnes, Parker and Barham, Paul and Michel, Paul and Jin,
                  Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala,
                  Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana,
                  Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and
                  Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and
                  Cogan, Sarah and Perrin, Sarah and Arnold, Sébastien M R and
                  Krause, Sebastian and Dai, Shengyang and Garg, Shruti and
                  Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan,
                  Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and
                  Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav,
                  Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and
                  Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun
                  and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao
                  and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao,
                  Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris
                  and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and
                  Hadsell, Raia and Sculley, D and Banks, Jeanine and Dragan,
                  Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and
                  Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement
                  and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel,
                  Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi,
                  Robert and Andreev, Alek},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-07-31},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {In this work, we introduce Gemma 2, a new addition to the
                  Gemma family of lightweight, state-of-the-art open models,
                  ranging in scale from 2 billion to 27 billion parameters. In
                  this new version, we apply several known technical
                  modifications to the Transformer architecture, such as
                  interleaving local-global attentions (Beltagy et al., 2020a)
                  and group-query attention (Ainslie et al., 2023). We also
                  train the 2B and 9B models with knowledge distillation (Hinton
                  et al., 2015) instead of next token prediction. The resulting
                  models deliver the best performance for their size, and even
                  offer competitive alternatives to models that are 2-3 times
                  bigger. We release all our models to the community.},
  urldate      = {2024-11-11}
}

@ARTICLE{Zoph-ST-MoE-DesigningModels-2022,
  title        = {{ST}-{MoE}: Designing Stable and transferable sparse expert
                  models},
  author       = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan
                  and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus,
                  William},
  journaltitle = {arXiv [cs.CL]},
  date         = {2022-02-17},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Scale has opened new frontiers in natural language processing
                  -- but at a high cost. In response, Mixture-of-Experts (MoE)
                  and Switch Transformers have been proposed as an energy
                  efficient path to even larger and more capable language
                  models. But advancing the state-of-the-art across a broad set
                  of natural language tasks has been hindered by training
                  instabilities and uncertain quality during fine-tuning. Our
                  work focuses on these issues and acts as a design guide. We
                  conclude by scaling a sparse model to 269B parameters, with a
                  computational cost comparable to a 32B dense encoder-decoder
                  Transformer (Stable and Transferable Mixture-of-Experts or
                  ST-MoE-32B). For the first time, a sparse model achieves
                  state-of-the-art performance in transfer learning, across a
                  diverse set of tasks including reasoning (SuperGLUE, ARC Easy,
                  ARC Challenge), summarization (XSum, CNN-DM), closed book
                  question answering (WebQA, Natural Questions), and
                  adversarially constructed tasks (Winogrande, ANLI R3).},
  urldate      = {2024-11-16}
}

@ARTICLE{Gale-MegaBlocks-EfficientMixture-of-Experts-2022,
  title        = {{MegaBlocks}: Efficient sparse training with
                  Mixture-of-Experts},
  author       = {Gale, Trevor and Narayanan, Deepak and Young, Cliff and
                  Zaharia, Matei},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-11-28},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {We present MegaBlocks, a system for efficient
                  Mixture-of-Experts (MoE) training on GPUs. Our system is
                  motivated by the limitations of current frameworks, which
                  restrict the dynamic routing in MoE layers to satisfy the
                  constraints of existing software and hardware. These
                  formulations force a tradeoff between model quality and
                  hardware efficiency, as users must choose between dropping
                  tokens from the computation or wasting computation and memory
                  on padding. To address these limitations, we reformulate MoE
                  computation in terms of block-sparse operations and develop
                  new block-sparse GPU kernels that efficiently handle the
                  dynamism present in MoEs. Our approach never drops tokens and
                  maps efficiently to modern hardware, enabling end-to-end
                  training speedups of up to 40\% over MoEs trained with the
                  state-of-the-art Tutel library and 2.4x over DNNs trained with
                  the highly-optimized Megatron-LM framework.},
  urldate      = {2024-11-16}
}

@ARTICLE{Yoran-AssistantBench-Web-2024,
  title        = {{AssistantBench}: Can web agents solve realistic and
                  time-consuming tasks?},
  author       = {Yoran, Ori and Amouyal, Samuel Joseph and Malaviya, Chaitanya
                  and Bogin, Ben and {Ofir Press} and Berant, Jonathan},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-07-22},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Language agents, built on top of language models (LMs), are
                  systems that can interact with complex environments, such as
                  the open web. In this work, we examine whether such agents can
                  perform realistic and time-consuming tasks on the web, e.g.,
                  monitoring real-estate markets or locating relevant nearby
                  businesses. We introduce AssistantBench, a challenging new
                  benchmark consisting of 214 realistic tasks that can be
                  automatically evaluated, covering different scenarios and
                  domains. We find that AssistantBench exposes the limitations
                  of current systems, including language models and
                  retrieval-augmented language models, as no model reaches an
                  accuracy of more than 26 points. While closed-book LMs perform
                  well in terms of accuracy, they exhibit low precision and tend
                  to hallucinate facts. State-of-the-art web agents reach a
                  score of near zero. Additionally, we introduce SeePlanAct
                  (SPA), a new web agent that significantly outperforms previous
                  agents, and an ensemble of SPA and closed-book models reaches
                  the best overall performance. Moreover, we analyze failures of
                  current systems and highlight that open web navigation remains
                  a major challenge.},
  urldate      = {2024-11-17}
}

@ARTICLE{Boisvert-WorkArena++-TowardsTasks-2024,
  title        = {{WorkArena++}: Towards compositional planning and
                  reasoning-based common knowledge work tasks},
  author       = {Boisvert, Léo and Thakkar, Megh and Gasse, Maxime and Caccia,
                  Massimo and De Chezelles, Thibault Le Sellier and Cappart,
                  Quentin and Chapados, Nicolas and Lacoste, Alexandre and
                  Drouin, Alexandre},
  journaltitle = {arXiv [cs.AI]},
  date         = {2024-07-07},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {The ability of large language models (LLMs) to mimic
                  human-like intelligence has led to a surge in LLM-based
                  autonomous agents. Though recent LLMs seem capable of planning
                  and reasoning given user instructions, their effectiveness in
                  applying these capabilities for autonomous task solving
                  remains underexplored. This is especially true in enterprise
                  settings, where automated agents hold the promise of a high
                  impact. To fill this gap, we propose WorkArena++, a novel
                  benchmark consisting of 682 tasks corresponding to realistic
                  workflows routinely performed by knowledge workers.
                  WorkArena++ is designed to evaluate the planning,
                  problem-solving, logical/arithmetic reasoning, retrieval, and
                  contextual understanding abilities of web agents. Our
                  empirical studies across state-of-the-art LLMs and
                  vision-language models (VLMs), as well as human workers,
                  reveal several challenges for such models to serve as useful
                  assistants in the workplace. In addition to the benchmark, we
                  provide a mechanism to effortlessly generate thousands of
                  ground-truth observation/action traces, which can be used for
                  fine-tuning existing models. Overall, we expect this work to
                  serve as a useful resource to help the community progress
                  toward capable autonomous agents. The benchmark can be found
                  at
                  https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus.},
  urldate      = {2024-11-17}
}

@ARTICLE{Murty-NNetscape-NavigatorDemonstrator-2024,
  title        = {{NNetscape} Navigator: Complex demonstrations for web agents
                  without a demonstrator},
  author       = {Murty, Shikhar and Bahdanau, Dzmitry and Manning, Christopher
                  D},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-10-03},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {We introduce NNetscape Navigator (NNetnav), a method for
                  training web agents entirely through synthetic demonstrations.
                  These demonstrations are collected by first interacting with a
                  browser to generate trajectory rollouts, which are then
                  retroactively labeled into instructions using a language
                  model. Most work on training browser agents has relied on
                  expensive human supervision, and the limited previous work on
                  such interaction-first synthetic data techniques has failed to
                  provide effective search through the exponential space of
                  exploration. In contrast, NNetnav exploits the hierarchical
                  structure of language instructions to make this search more
                  tractable: complex instructions are typically decomposable
                  into simpler subtasks, allowing NNetnav to automatically prune
                  interaction episodes when an intermediate trajectory cannot be
                  annotated with a meaningful sub-task. We use NNetnav
                  demonstrations from a language model for supervised
                  fine-tuning of a smaller language model policy, and find
                  improvements of 6 points on WebArena and over 20 points on
                  MiniWoB++, two popular environments for web-agents. Notably,
                  on WebArena, we observe that language model policies can be
                  further enhanced when fine-tuned with NNetnav demonstrations
                  derived from the same language model. Finally, we collect and
                  release a dataset of over 6k NNetnav demonstrations on
                  WebArena, spanning a diverse and complex set of instructions.},
  urldate      = {2024-11-17}
}

@ARTICLE{Gu-Efficiently-ModelingSpaces-2021,
  title        = {Efficiently modeling long sequences with structured state
                  spaces},
  author       = {Gu, Albert and Goel, Karan and Ré, Christopher},
  journaltitle = {arXiv [cs.LG]},
  date         = {2021-10-30},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {A central goal of sequence modeling is designing a single
                  principled model that can address sequence data across a range
                  of modalities and tasks, particularly on long-range
                  dependencies. Although conventional models including RNNs,
                  CNNs, and Transformers have specialized variants for capturing
                  long dependencies, they still struggle to scale to very long
                  sequences of $10000$ or more steps. A promising recent
                  approach proposed modeling sequences by simulating the
                  fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t),
                  y(t) = Cx(t) + Du(t) \), and showed that for appropriate
                  choices of the state matrix \( A \), this system could handle
                  long-range dependencies mathematically and empirically.
                  However, this method has prohibitive computation and memory
                  requirements, rendering it infeasible as a general sequence
                  modeling solution. We propose the Structured State Space
                  sequence model (S4) based on a new parameterization for the
                  SSM, and show that it can be computed much more efficiently
                  than prior approaches while preserving their theoretical
                  strengths. Our technique involves conditioning \( A \) with a
                  low-rank correction, allowing it to be diagonalized stably and
                  reducing the SSM to the well-studied computation of a Cauchy
                  kernel. S4 achieves strong empirical results across a diverse
                  range of established benchmarks, including (i) 91\% accuracy
                  on sequential CIFAR-10 with no data augmentation or auxiliary
                  losses, on par with a larger 2-D ResNet, (ii) substantially
                  closing the gap to Transformers on image and language modeling
                  tasks, while performing generation $60\times$ faster (iii)
                  SoTA on every task from the Long Range Arena benchmark,
                  including solving the challenging Path-X task of length 16k
                  that all prior work fails on, while being as efficient as all
                  competitors.},
  urldate      = {2024-11-18}
}

@ARTICLE{Peng-RWKV-ReinventingEra-2023,
  title        = {{RWKV}: Reinventing {RNNs} for the Transformer era},
  author       = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak,
                  Alon and Arcadinho, Samuel and Biderman, Stella and Cao,
                  Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo
                  and Gv, Kranthi Kiran and He, Xuzheng and Hou, Haowen and Lin,
                  Jiaju and Kazienko, Przemyslaw and Kocon, Jan and Kong,
                  Jiaming and Koptyra, Bartlomiej and Lau, Hayden and Mantri,
                  Krishna Sri Ipsit and Mom, Ferdinand and Saito, Atsushi and
                  Song, Guangyu and Tang, Xiangru and Wang, Bolun and Wind,
                  Johan S and Wozniak, Stanislaw and Zhang, Ruichong and Zhang,
                  Zhenyuan and Zhao, Qihang and Zhou, Peng and Zhou, Qinghua and
                  Zhu, Jian and Zhu, Rui-Jie},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-05-22},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Transformers have revolutionized almost all natural language
                  processing (NLP) tasks but suffer from memory and
                  computational complexity that scales quadratically with
                  sequence length. In contrast, recurrent neural networks (RNNs)
                  exhibit linear scaling in memory and computational
                  requirements but struggle to match the same performance as
                  Transformers due to limitations in parallelization and
                  scalability. We propose a novel model architecture, Receptance
                  Weighted Key Value (RWKV), that combines the efficient
                  parallelizable training of transformers with the efficient
                  inference of RNNs. Our approach leverages a linear attention
                  mechanism and allows us to formulate the model as either a
                  Transformer or an RNN, thus parallelizing computations during
                  training and maintains constant computational and memory
                  complexity during inference. We scale our models as large as
                  14 billion parameters, by far the largest dense RNN ever
                  trained, and find RWKV performs on par with similarly sized
                  Transformers, suggesting future work can leverage this
                  architecture to create more efficient models. This work
                  presents a significant step towards reconciling trade-offs
                  between computational efficiency and model performance in
                  sequence processing tasks.},
  urldate      = {2024-11-18}
}

@ARTICLE{Dubey-Llama-3Models-2024,
  title        = {The Llama 3 herd of models},
  author       = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and
                  Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and
                  Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela
                  and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and
                  Mitra, Archi and Sravankumar, Archie and Korenev, Artem and
                  Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez,
                  Aurelien and Gregerson, Austen and Spataru, Ava and Roziere,
                  Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie
                  and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and
                  Marra, Chris and McConnell, Chris and Keller, Christian and
                  Touret, Christophe and Wu, Chunyang and Wong, Corinne and
                  Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius,
                  Damien and Song, Daniel and Pintz, Danielle and Livshits,
                  Danny and Esiobu, David and Choudhary, Dhruv and Mahajan,
                  Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes,
                  Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova,
                  Elina and Dinan, Emily and Smith, Eric Michael and Radenovic,
                  Filip and Zhang, Frank and Synnaeve, Gabriel and Lee,
                  Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and
                  Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and
                  Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron,
                  Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and
                  Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet,
                  Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and
                  Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der
                  Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee,
                  Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and
                  Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna
                  and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and
                  Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala,
                  Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and
                  Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini,
                  Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley
                  and Bhalla, Kunal and Rantala-Yeary, Lauren and van der
                  Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins,
                  Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and
                  Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and
                  Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and
                  Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and
                  Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and
                  Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan,
                  Mona and Goyal, Naman and Torabi, Narjes and Bashlykov,
                  Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and
                  Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and
                  Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng,
                  Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan,
                  Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and
                  Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and
                  Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic,
                  Robert and Raileanu, Roberta and Girdhar, Rohit and Patel,
                  Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly,
                  Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang,
                  Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh,
                  Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov,
                  Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy,
                  Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti
                  and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and
                  Whitman, Spencer and Sootla, Sten and Collot, Stephane and
                  Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar
                  and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and
                  Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor
                  and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and
                  Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and
                  Gonguet, Vincent and Do, Virginie and Vogeti, Vish and
                  Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu,
                  Wenyin and Meers, Whitney and Martinet, Xavier and Wang,
                  Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia,
                  Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur,
                  Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and
                  Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert,
                  Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and
                  Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and
                  Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi,
                  Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon,
                  Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex
                  and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda
                  and Sangani, Amit and Yunus, Anam and Lupu, Andrei and
                  Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho,
                  Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani,
                  Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury,
                  Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and
                  Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer,
                  Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth
                  and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and
                  Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and
                  Spence, Brandon and Stojkovic, Brani and Gamido, Brian and
                  Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia,
                  Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao
                  and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and
                  Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon
                  and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt,
                  Danny and Adkins, David and Xu, David and Testuggine, Davide
                  and David, Delia and Parikh, Devi and Liskovich, Diana and
                  Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin
                  and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine
                  and Presani, Eleonora and Hahn, Emily and Wood, Emily and
                  Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and
                  Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng
                  and Ozgenel, Firat and Caggioni, Francesco and Guzmán,
                  Francisco and Kanayet, Frank and Seide, Frank and Florez,
                  Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and
                  Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman,
                  Grant and Sizov, Grigory and {Guangyi} and {Zhang} and
                  Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and
                  Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph,
                  Harrison and Suk, Helen and Aspegren, Henry and Goldman,
                  Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor
                  and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and
                  Geboski, James and Kohli, James and Asher, Japhet and Gaya,
                  Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan,
                  Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul,
                  Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and
                  Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie,
                  Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang,
                  Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and
                  Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun
                  and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena,
                  Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and
                  Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg,
                  Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and
                  Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich,
                  Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani,
                  Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus,
                  Martynas and Hasson, Matan and Lennie, Matthew and Reso,
                  Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya
                  and Keneally, Meghan and Seltzer, Michael L and Valko, Michal
                  and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and
                  Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang,
                  Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari,
                  Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks,
                  Natascha and White, Natasha and Bawa, Navyata and Singhal,
                  Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay
                  Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and
                  Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and
                  Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab,
                  Paul and Balaji, Pavan and Rittner, Pedro and Bontrager,
                  Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina
                  and Ratanchandani, Prashant and Yuvraj, Pritish and Liang,
                  Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and
                  Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li,
                  Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky
                  and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and
                  Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt,
                  Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru
                  and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh
                  and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin,
                  Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang,
                  Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal,
                  Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max,
                  Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield,
                  Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho,
                  Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury,
                  Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and
                  Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li,
                  Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy
                  and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria
                  and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay
                  Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad
                  and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov,
                  Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and
                  Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang,
                  Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and
                  Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia,
                  Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying
                  and Adi, Yossi and Nam, Youngjin and {Yu} and {Wang} and Hao,
                  Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito,
                  Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu
                  and Zhao, Zhiwei},
  journaltitle = {arXiv [cs.AI]},
  date         = {2024-07-31},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {Modern artificial intelligence (AI) systems are powered by
                  foundation models. This paper presents a new set of foundation
                  models, called Llama 3. It is a herd of language models that
                  natively support multilinguality, coding, reasoning, and tool
                  usage. Our largest model is a dense Transformer with 405B
                  parameters and a context window of up to 128K tokens. This
                  paper presents an extensive empirical evaluation of Llama 3.
                  We find that Llama 3 delivers comparable quality to leading
                  language models such as GPT-4 on a plethora of tasks. We
                  publicly release Llama 3, including pre-trained and
                  post-trained versions of the 405B parameter language model and
                  our Llama Guard 3 model for input and output safety. The paper
                  also presents the results of experiments in which we integrate
                  image, video, and speech capabilities into Llama 3 via a
                  compositional approach. We observe this approach performs
                  competitively with the state-of-the-art on image, video, and
                  speech recognition tasks. The resulting models are not yet
                  being broadly released as they are still under development.},
  urldate      = {2024-09-03}
}

@ARTICLE{Yu-Orca-DistributedModels-2022,
  title        = {Orca: A distributed serving system for Transformer-based
                  generative models},
  author       = {Yu, Gyeong-In and Jeong, Joo Seong},
  journaltitle = {Oper Syst Des Implement},
  pages        = {521--538},
  date         = {2022},
  abstract     = {Large-scale Transformer-based models trained for generation
                  tasks (e.g., GPT-3) have recently attracted huge interest,
                  emphasizing the need for system support for serving models in
                  this family. Since these models generate a next token in an
                  autoregressive manner, one has to run the model multiple times
                  to process an inference request where each iteration of the
                  model generates a single output token for the request.
                  However, existing systems for inference serving do not perform
                  well on this type of workload that has a multi-iteration
                  characteristic, due to their inflexible scheduling mechanism
                  that cannot change the current batch of requests being
                  processed; requests that have finished earlier than other
                  requests in a batch cannot return to the client, while newly
                  arrived requests have to wait until the current batch
                  completely finishes. In this paper, we propose iteration-level
                  scheduling, a new scheduling mechanism that schedules
                  execution at the granularity of iteration (instead of request)
                  where the scheduler invokes the execution engine to run only a
                  single iteration of the model on the batch. In addition, to
                  apply batching and iteration-level scheduling to a Transformer
                  model at the same time, we suggest selective batching, which
                  applies batching only to a selected set of operations. Based
                  on these two techniques, we have implemented a distributed
                  serving system called ORCA, with additional designs for
                  scalability to models with hundreds of billions of parameters.
                  Our evaluation on a GPT-3 175B model shows that ORCA can
                  significantly outperform NVIDIA FasterTransformer in terms of
                  both latency and throughput: 36.9× throughput improvement at
                  the same level of latency.},
  urldate      = {2024-09-09}
}

@ARTICLE{Agrawal-Taming-Throughput-latencySarathi-serve-2024,
  title        = {Taming throughput-latency tradeoff in {LLM} inference with
                  sarathi-serve},
  author       = {Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan,
                  Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and
                  Tumanov, Alexey and Ramjee, Ramachandran},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-03-04},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Each LLM serving request goes through two phases. The first is
                  prefill which processes the entire input prompt and produces
                  the first output token and the second is decode which
                  generates the rest of output tokens, one-at-a-time. Prefill
                  iterations have high latency but saturate GPU compute due to
                  parallel processing of the input prompt. In contrast, decode
                  iterations have low latency but also low compute utilization
                  because a decode iteration processes only a single token per
                  request. This makes batching highly effective for decodes and
                  consequently for overall throughput. However, batching
                  multiple requests leads to an interleaving of prefill and
                  decode iterations which makes it challenging to achieve both
                  high throughput and low latency. We introduce an efficient LLM
                  inference scheduler, Sarathi-Serve, to address this
                  throughput-latency tradeoff. Sarathi-Serve introduces
                  chunked-prefills which splits a prefill request into near
                  equal sized chunks and creates stall-free schedules that adds
                  new requests in a batch without pausing ongoing decodes.
                  Stall-free scheduling unlocks the opportunity to improve
                  throughput with large batch sizes while minimizing the effect
                  of batching on latency. Furthermore, uniform batches in
                  Sarathi-Serve ameliorate the imbalance between iterations
                  resulting in minimal pipeline bubbles. Our techniques yield
                  significant improvements in inference performance across
                  models and hardware under tail latency constraints. For
                  Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving
                  capacity and up to 3.7x higher serving capacity for the Yi-34B
                  model on two A100 GPUs as compared to vLLM. When used with
                  pipeline parallelism on Falcon-180B, Sarathi-Serve provides up
                  to 5.6x gain in the end-to-end serving capacity. The source
                  code for Sarathi-Serve is available at
                  https://github.com/microsoft/sarathi-serve.},
  urldate      = {2024-09-09}
}

@ARTICLE{Khattab-DSPy-CompilingPipelines-2023,
  title        = {{DSPy}: Compiling declarative language model calls into
                  self-improving pipelines},
  author       = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and
                  Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and
                  Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T and
                  Moazam, Hanna and Miller, Heather and Zaharia, Matei and
                  Potts, Christopher},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-05},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {The ML community is rapidly exploring techniques for prompting
                  language models (LMs) and for stacking them into pipelines
                  that solve complex tasks. Unfortunately, existing LM pipelines
                  are typically implemented using hard-coded "prompt templates",
                  i.e. lengthy strings discovered via trial and error. Toward a
                  more systematic approach for developing and optimizing LM
                  pipelines, we introduce DSPy, a programming model that
                  abstracts LM pipelines as text transformation graphs, i.e.
                  imperative computational graphs where LMs are invoked through
                  declarative modules. DSPy modules are parameterized, meaning
                  they can learn (by creating and collecting demonstrations) how
                  to apply compositions of prompting, finetuning, augmentation,
                  and reasoning techniques. We design a compiler that will
                  optimize any DSPy pipeline to maximize a given metric. We
                  conduct two case studies, showing that succinct DSPy programs
                  can express and optimize sophisticated LM pipelines that
                  reason about math word problems, tackle multi-hop retrieval,
                  answer complex questions, and control agent loops. Within
                  minutes of compiling, a few lines of DSPy allow GPT-3.5 and
                  llama2-13b-chat to self-bootstrap pipelines that outperform
                  standard few-shot prompting (generally by over 25\% and 65\%,
                  respectively) and pipelines with expert-created demonstrations
                  (by up to 5-46\% and 16-40\%, respectively). On top of that,
                  DSPy programs compiled to open and relatively small LMs like
                  770M-parameter T5 and llama2-13b-chat are competitive with
                  approaches that rely on expert-written prompt chains for
                  proprietary GPT-3.5. DSPy is available at
                  https://github.com/stanfordnlp/dspy},
  urldate      = {2024-09-23}
}

@ARTICLE{Packer-MemGPT-TowardsSystems-2023,
  title        = {{MemGPT}: Towards {LLMs} as operating systems},
  author       = {Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang,
                  Vivian and Patil, Shishir G and Stoica, Ion and Gonzalez,
                  Joseph E},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-10-12},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {Large language models (LLMs) have revolutionized AI, but are
                  constrained by limited context windows, hindering their
                  utility in tasks like extended conversations and document
                  analysis. To enable using context beyond limited context
                  windows, we propose virtual context management, a technique
                  drawing inspiration from hierarchical memory systems in
                  traditional operating systems that provide the appearance of
                  large memory resources through data movement between fast and
                  slow memory. Using this technique, we introduce MemGPT
                  (Memory-GPT), a system that intelligently manages different
                  memory tiers in order to effectively provide extended context
                  within the LLM's limited context window, and utilizes
                  interrupts to manage control flow between itself and the user.
                  We evaluate our OS-inspired design in two domains where the
                  limited context windows of modern LLMs severely handicaps
                  their performance: document analysis, where MemGPT is able to
                  analyze large documents that far exceed the underlying LLM's
                  context window, and multi-session chat, where MemGPT can
                  create conversational agents that remember, reflect, and
                  evolve dynamically through long-term interactions with their
                  users. We release MemGPT code and data for our experiments at
                  https://memgpt.ai.},
  urldate      = {2024-09-23}
}

@ARTICLE{Yang-Tensor-ProgramsTransfer-2022,
  title        = {Tensor programs {V}: Tuning large neural networks via
                  zero-shot hyperparameter transfer},
  author       = {Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor,
                  Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and
                  Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-03-07},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Hyperparameter (HP) tuning in deep learning is an expensive
                  process, prohibitively so for neural networks (NNs) with
                  billions of parameters. We show that, in the recently
                  discovered Maximal Update Parametrization (muP), many optimal
                  HPs remain stable even as model size changes. This leads to a
                  new HP tuning paradigm we call muTransfer: parametrize the
                  target model in muP, tune the HP indirectly on a smaller
                  model, and zero-shot transfer them to the full-sized model,
                  i.e., without directly tuning the latter at all. We verify
                  muTransfer on Transformer and ResNet. For example, 1) by
                  transferring pretraining HPs from a model of 13M parameters,
                  we outperform published numbers of BERT-large (350M
                  parameters), with a total tuning cost equivalent to
                  pretraining BERT-large once; 2) by transferring from 40M
                  parameters, we outperform published numbers of the 6.7B GPT-3
                  model, with tuning cost only 7\% of total pretraining cost. A
                  Pytorch implementation of our technique can be found at
                  github.com/microsoft/mup and installable via `pip install
                  mup`.},
  urldate      = {2024-10-14}
}

@ARTICLE{Kaplan-Scaling-LawsModels-2020,
  title        = {Scaling laws for neural language models},
  author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown,
                  Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and
                  Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journaltitle = {arXiv [cs.LG]},
  date         = {2020-01-22},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {We study empirical scaling laws for language model performance
                  on the cross-entropy loss. The loss scales as a power-law with
                  model size, dataset size, and the amount of compute used for
                  training, with some trends spanning more than seven orders of
                  magnitude. Other architectural details such as network width
                  or depth have minimal effects within a wide range. Simple
                  equations govern the dependence of overfitting on
                  model/dataset size and the dependence of training speed on
                  model size. These relationships allow us to determine the
                  optimal allocation of a fixed compute budget. Larger models
                  are significantly more sample-efficient, such that optimally
                  compute-efficient training involves training very large models
                  on a relatively modest amount of data and stopping
                  significantly before convergence.},
  urldate      = {2024-10-14}
}

@ARTICLE{Hendrycks-Measuring-MassiveUnderstanding-2020,
  title        = {Measuring massive multitask language understanding},
  author       = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou,
                  Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journaltitle = {arXiv [cs.CY]},
  date         = {2020-09-07},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CY},
  abstract     = {We propose a new test to measure a text model's multitask
                  accuracy. The test covers 57 tasks including elementary
                  mathematics, US history, computer science, law, and more. To
                  attain high accuracy on this test, models must possess
                  extensive world knowledge and problem solving ability. We find
                  that while most recent models have near random-chance
                  accuracy, the very largest GPT-3 model improves over random
                  chance by almost 20 percentage points on average. However, on
                  every one of the 57 tasks, the best models still need
                  substantial improvements before they can reach expert-level
                  accuracy. Models also have lopsided performance and frequently
                  do not know when they are wrong. Worse, they still have
                  near-random accuracy on some socially important subjects such
                  as morality and law. By comprehensively evaluating the breadth
                  and depth of a model's academic and professional
                  understanding, our test can be used to analyze models across
                  many tasks and to identify important shortcomings.},
  urldate      = {2024-09-16}
}

@ARTICLE{Oren-Proving-TestModels-2023,
  title        = {Proving test set contamination in black box language models},
  author       = {Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and
                  Ladhak, Faisal and Hashimoto, Tatsunori B},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-26},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Large language models are trained on vast amounts of internet
                  data, prompting concerns and speculation that they have
                  memorized public benchmarks. Going from speculation to proof
                  of contamination is challenging, as the pretraining data used
                  by proprietary models are often not publicly accessible. We
                  show that it is possible to provide provable guarantees of
                  test set contamination in language models without access to
                  pretraining data or model weights. Our approach leverages the
                  fact that when there is no data contamination, all orderings
                  of an exchangeable benchmark should be equally likely. In
                  contrast, the tendency for language models to memorize example
                  order means that a contaminated language model will find
                  certain canonical orderings to be much more likely than
                  others. Our test flags potential contamination whenever the
                  likelihood of a canonically ordered benchmark dataset is
                  significantly higher than the likelihood after shuffling the
                  examples. We demonstrate that our procedure is sensitive
                  enough to reliably prove test set contamination in challenging
                  situations, including models as small as 1.4 billion
                  parameters, on small test sets of only 1000 examples, and
                  datasets that appear only a few times in the pretraining
                  corpus. Using our test, we audit five popular publicly
                  accessible language models for test set contamination and find
                  little evidence for pervasive contamination.},
  urldate      = {2024-09-16}
}

@ARTICLE{Oehrn-Chronic-AdaptiveTrial-2024,
  title        = {Chronic adaptive deep brain stimulation versus conventional
                  stimulation in Parkinson's disease: a blinded randomized
                  feasibility trial},
  author       = {Oehrn, Carina R and Cernera, Stephanie and Hammer, Lauren H
                  and Shcherbakova, Maria and Yao, Jiaang and Hahn, Amelia and
                  Wang, Sarah and Ostrem, Jill L and Little, Simon and Starr,
                  Philip A},
  journaltitle = {Nat. Med.},
  publisher    = {Springer Science and Business Media LLC},
  pages        = {1--12},
  date         = {2024-08-19},
  abstract     = {Deep brain stimulation (DBS) is a widely used therapy for
                  Parkinson's disease (PD) but lacks dynamic responsiveness to
                  changing clinical and neural states. Feedback control might
                  improve therapeutic effectiveness, but the optimal control
                  strategy and additional benefits of 'adaptive'
                  neurostimulation are unclear. Here we present the results of a
                  blinded randomized cross-over pilot trial aimed at determining
                  the neural correlates of specific motor signs in individuals
                  with PD and the feasibility of using these signals to drive
                  adaptive DBS. Four male patients with PD were recruited from a
                  population undergoing DBS implantation for motor fluctuations,
                  with each patient receiving adaptive DBS and continuous DBS.
                  We identified stimulation-entrained gamma oscillations in the
                  subthalamic nucleus or motor cortex as optimal markers of high
                  versus low dopaminergic states and their associated residual
                  motor signs in all four patients. We then demonstrated
                  improved motor symptoms and quality of life with adaptive
                  compared to clinically optimized standard stimulation. The
                  results of this pilot trial highlight the promise of
                  personalized adaptive neurostimulation in PD based on
                  data-driven selection of neural signals. Furthermore, these
                  findings provide the foundation for further larger clinical
                  trials to evaluate the efficacy of personalized adaptive
                  neurostimulation in PD and other neurological disorders.
                  ClinicalTrials.gov registration: NCT03582891 .},
  urldate      = {2024-09-20},
  language     = {en}
}

@ARTICLE{Malkov-Efficient-RobustGraphs-2020,
  title        = {Efficient and robust approximate nearest neighbor search using
                  hierarchical Navigable Small World graphs},
  author       = {Malkov, Yu A and Yashunin, D A},
  journaltitle = {IEEE Trans. Pattern Anal. Mach. Intell.},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume       = {42},
  issue        = {4},
  pages        = {824--836},
  date         = {2020-04},
  abstract     = {We present a new approach for the approximate K-nearest
                  neighbor search based on navigable small world graphs with
                  controllable hierarchy (Hierarchical NSW, HNSW). The proposed
                  solution is fully graph-based, without any need for additional
                  search structures (typically used at the coarse search stage
                  of the most proximity graph techniques). Hierarchical NSW
                  incrementally builds a multi-layer structure consisting of a
                  hierarchical set of proximity graphs (layers) for nested
                  subsets of the stored elements. The maximum layer in which an
                  element is present is selected randomly with an exponentially
                  decaying probability distribution. This allows producing
                  graphs similar to the previously studied Navigable Small World
                  (NSW) structures while additionally having the links separated
                  by their characteristic distance scales. Starting the search
                  from the upper layer together with utilizing the scale
                  separation boosts the performance compared to NSW and allows a
                  logarithmic complexity scaling. Additional employment of a
                  heuristic for selecting proximity graph neighbors
                  significantly increases performance at high recall and in case
                  of highly clustered data. Performance evaluation has
                  demonstrated that the proposed general metric space search
                  index is able to strongly outperform previous opensource
                  state-of-the-art vector-only approaches. Similarity of the
                  algorithm to the skip list structure allows straightforward
                  balanced distributed implementation.},
  urldate      = {2024-09-21},
  language     = {en}
}

@ARTICLE{Santhanam-ColBERTv2-EffectiveInteraction-2021,
  title        = {{ColBERTv2}: Effective and efficient retrieval via lightweight
                  late interaction},
  author       = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and
                  Potts, Christopher and Zaharia, Matei},
  journaltitle = {arXiv [cs.IR]},
  date         = {2021-12-02},
  eprinttype   = {arXiv},
  eprintclass  = {cs.IR},
  abstract     = {Neural information retrieval (IR) has greatly advanced search
                  and other knowledge-intensive language tasks. While many
                  neural IR methods encode queries and documents into
                  single-vector representations, late interaction models produce
                  multi-vector representations at the granularity of each token
                  and decompose relevance modeling into scalable token-level
                  computations. This decomposition has been shown to make late
                  interaction more effective, but it inflates the space
                  footprint of these models by an order of magnitude. In this
                  work, we introduce ColBERTv2, a retriever that couples an
                  aggressive residual compression mechanism with a denoised
                  supervision strategy to simultaneously improve the quality and
                  space footprint of late interaction. We evaluate ColBERTv2
                  across a wide range of benchmarks, establishing
                  state-of-the-art quality within and outside the training
                  domain while reducing the space footprint of late interaction
                  models by 6--10$\times$.},
  urldate      = {2024-09-21}
}

@ARTICLE{Chen-Accelerating-LargeSampling-2023,
  title        = {Accelerating large language model decoding with speculative
                  sampling},
  author       = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and
                  Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-02-02},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {We present speculative sampling, an algorithm for accelerating
                  transformer decoding by enabling the generation of multiple
                  tokens from each transformer call. Our algorithm relies on the
                  observation that the latency of parallel scoring of short
                  continuations, generated by a faster but less powerful draft
                  model, is comparable to that of sampling a single token from
                  the larger target model. This is combined with a novel
                  modified rejection sampling scheme which preserves the
                  distribution of the target model within hardware numerics. We
                  benchmark speculative sampling with Chinchilla, a 70 billion
                  parameter language model, achieving a 2-2.5x decoding speedup
                  in a distributed setup, without compromising the sample
                  quality or making modifications to the model itself.},
  urldate      = {2024-09-14}
}

@ARTICLE{Cai-Medusa-SimpleHeads-2024,
  title        = {Medusa: Simple {LLM} inference acceleration framework with
                  multiple decoding heads},
  author       = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng,
                  Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-01-19},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Large Language Models (LLMs) employ auto-regressive decoding
                  that requires sequential computation, with each step reliant
                  on the previous one's output. This creates a bottleneck as
                  each step necessitates moving the full model parameters from
                  High-Bandwidth Memory (HBM) to the accelerator's cache. While
                  methods such as speculative decoding have been suggested to
                  address this issue, their implementation is impeded by the
                  challenges associated with acquiring and maintaining a
                  separate draft model. In this paper, we present Medusa, an
                  efficient method that augments LLM inference by adding extra
                  decoding heads to predict multiple subsequent tokens in
                  parallel. Using a tree-based attention mechanism, Medusa
                  constructs multiple candidate continuations and verifies them
                  simultaneously in each decoding step. By leveraging parallel
                  processing, Medusa substantially reduces the number of
                  decoding steps required. We present two levels of fine-tuning
                  procedures for Medusa to meet the needs of different use
                  cases: Medusa-1: Medusa is directly fine-tuned on top of a
                  frozen backbone LLM, enabling lossless inference acceleration.
                  Medusa-2: Medusa is fine-tuned together with the backbone LLM,
                  enabling better prediction accuracy of Medusa heads and higher
                  speedup but needing a special training recipe that preserves
                  the backbone model's capabilities. Moreover, we propose
                  several extensions that improve or expand the utility of
                  Medusa, including a self-distillation to handle situations
                  where no training data is available and a typical acceptance
                  scheme to boost the acceptance rate while maintaining
                  generation quality. We evaluate Medusa on models of various
                  sizes and training procedures. Our experiments demonstrate
                  that Medusa-1 can achieve over 2.2x speedup without
                  compromising generation quality, while Medusa-2 further
                  improves the speedup to 2.3-3.6x.},
  urldate      = {2024-09-14}
}

@INPROCEEDINGS{Schulman-Finding-LocallyOptimization-2013,
  title     = {Finding locally optimal, collision-free trajectories with
               sequential convex optimization},
  author    = {Schulman, John and Ho, Jonathan and Lee, Alex and Awwal, Ibrahim
               and Bradlow, Henry and Abbeel, Pieter},
  publisher = {Robotics: Science and Systems Foundation},
  date      = {2013-06-23},
  abstract  = {We present a novel approach for incorporating collision avoidance
               into trajectory optimization as a method of solving robotic
               motion planning problems. At the core of our approach are (i) A
               sequential convex optimization procedure, which penalizes
               collisions with a hinge loss and increases the penalty
               coefficients in an outer loop as necessary. (ii) An efficient
               formulation of the no-collisions constraint that directly
               considers continuous-time safety and enables the algorithm to
               reliably solve motion planning problems, including problems
               involving thin and complex obstacles. We benchmarked our
               algorithm against several other motion planning algorithms,
               solving a suite of 7-degree-of-freedom (DOF) arm-planning
               problems and 18-DOF full-body planning problems. We compared
               against sampling-based planners from OMPL, and we also compared
               to CHOMP, a leading approach for trajectory optimization. Our
               algorithm was faster than the alternatives, solved more problems,
               and yielded higher quality paths. Experimental evaluation on the
               following additional problem types also confirmed the speed and
               effectiveness of our approach: (i) Planning foot placements with
               34 degrees of freedom (28 joints + 6 DOF pose) of the Atlas
               humanoid robot as it maintains static stability and has to
               negotiate environmental constraints. (ii) Industrial box picking.
               (iii) Real-world motion planning for the PR2 that requires
               considering all degrees of freedom at the same time.}
}

@ARTICLE{Yao-WebShop-TowardsAgents-2022,
  title        = {{WebShop}: Towards scalable real-world web interaction with
                  grounded language agents},
  author       = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan,
                  Karthik},
  journaltitle = {arXiv [cs.CL]},
  date         = {2022-07-04},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Existing benchmarks for grounding language in interactive
                  environments either lack real-world linguistic elements, or
                  prove difficult to scale up due to substantial human
                  involvement in the collection of data or feedback signals. To
                  bridge this gap, we develop WebShop -- a simulated e-commerce
                  website environment with $1.18$ million real-world products
                  and $12,087$ crowd-sourced text instructions. Given a text
                  instruction specifying a product requirement, an agent needs
                  to navigate multiple types of webpages and issue diverse
                  actions to find, customize, and purchase an item. WebShop
                  provides several challenges for language grounding including
                  understanding compositional instructions, query
                  (re-)formulation, comprehending and acting on noisy text in
                  webpages, and performing strategic exploration. We collect
                  over $1,600$ human demonstrations for the task, and train and
                  evaluate a diverse range of agents using reinforcement
                  learning, imitation learning, and pre-trained image and
                  language models. Our best model achieves a task success rate
                  of $29\%$, which outperforms rule-based heuristics ($9.6\%$)
                  but is far lower than human expert performance ($59\%$). We
                  also analyze agent and human trajectories and ablate various
                  model components to provide insights for developing future
                  agents with stronger language understanding and decision
                  making abilities. Finally, we show that agents trained on
                  WebShop exhibit non-trivial sim-to-real transfer when
                  evaluated on amazon.com and ebay.com, indicating the potential
                  value of WebShop in developing practical web-based agents that
                  can operate in the wild.},
  urldate      = {2024-09-15}
}

@INPROCEEDINGS{Kraska-Case-LearnedStructures-2018,
  title      = {The Case for Learned Index Structures},
  author     = {Kraska, Tim and Beutel, Alex and Chi, Ed H and Dean, Jeffrey and
                Polyzotis, Neoklis},
  booktitle  = {Proceedings of the 2018 International Conference on Management
                of Data},
  publisher  = {ACM},
  location   = {New York, NY, USA},
  eventtitle = {SIGMOD/PODS '18: International Conference on Management of Data},
  venue      = {Houston TX USA},
  date       = {2018-05-27},
  urldate    = {2024-09-23},
  language   = {en}
}

@ARTICLE{Yancheng-Learning-CooperateAgents-2024,
  title        = {Learning to Cooperate with Humans using Generative Agents},
  author       = {Yancheng, Liang and Daphne, Chen and Abhishek, Gupta and
                  Simon, S Du and Natasha, Jaques},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-11-21},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Training agents that can coordinate zero-shot with humans is a
                  key mission in multi-agent reinforcement learning (MARL).
                  Current algorithms focus on training simulated human partner
                  policies which are then used to train a Cooperator agent. The
                  simulated human is produced either through behavior cloning
                  over a dataset of human cooperation behavior, or by using MARL
                  to create a population of simulated agents. However, these
                  approaches often struggle to produce a Cooperator that can
                  coordinate well with real humans, since the simulated humans
                  fail to cover the diverse strategies and styles employed by
                  people in the real world. We show \emph{learning a generative
                  model of human partners} can effectively address this issue.
                  Our model learns a latent variable representation of the human
                  that can be regarded as encoding the human's unique strategy,
                  intention, experience, or style. This generative model can be
                  flexibly trained from any (human or neural policy) agent
                  interaction data. By sampling from the latent space, we can
                  use the generative model to produce different partners to
                  train Cooperator agents. We evaluate our method --
                  \textbf{G}enerative \textbf{A}gent \textbf{M}odeling for
                  \textbf{M}ulti-agent \textbf{A}daptation (GAMMA) -- on
                  Overcooked, a challenging cooperative cooking game that has
                  become a standard benchmark for zero-shot coordination. We
                  conduct an evaluation with real human teammates, and the
                  results show that GAMMA consistently improves performance,
                  whether the generative model is trained on simulated
                  populations or human datasets. Further, we propose a method
                  for posterior sampling from the generative model that is
                  biased towards the human data, enabling us to efficiently
                  improve performance with only a small amount of expensive
                  human interaction data.},
  urldate      = {2024-11-22}
}

@ARTICLE{Zhang-H-_2-O-Heavy-HitterModels-2023,
  title        = {{H}$_$\_{2$}${O}: Heavy-Hitter Oracle for efficient generative
                  inference of large Language Models},
  author       = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen,
                  Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and
                  Tian, Yuandong and Ré, Christopher and Barrett, Clark and
                  Wang, Zhangyang and Chen, Beidi},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-06-24},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Large Language Models (LLMs), despite their recent impressive
                  accomplishments, are notably cost-prohibitive to deploy,
                  particularly for applications involving long-content
                  generation, such as dialogue systems and story writing. Often,
                  a large amount of transient state information, referred to as
                  the KV cache, is stored in GPU memory in addition to model
                  parameters, scaling linearly with the sequence length and
                  batch size. In this paper, we introduce a novel approach for
                  implementing the KV cache which significantly reduces its
                  memory footprint. Our approach is based on the noteworthy
                  observation that a small portion of tokens contributes most of
                  the value when computing attention scores. We call these
                  tokens Heavy Hitters (H$_2$). Through a comprehensive
                  investigation, we find that (i) the emergence of H$_2$ is
                  natural and strongly correlates with the frequent
                  co-occurrence of tokens in the text, and (ii) removing them
                  results in significant performance degradation. Based on these
                  insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache
                  eviction policy that dynamically retains a balance of recent
                  and H$_2$ tokens. We formulate the KV cache eviction as a
                  dynamic submodular problem and prove (under mild assumptions)
                  a theoretical guarantee for our novel eviction algorithm which
                  could help guide future work. We validate the accuracy of our
                  algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of
                  tasks. Our implementation of H$_2$O with 20\% heavy hitters
                  improves the throughput over three leading inference systems
                  DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen
                  by up to 29$\times$, 29$\times$, and 3$\times$ on OPT-6.7B and
                  OPT-30B. With the same batch size, H2O can reduce the latency
                  by up to 1.9$\times$. The code is available at
                  https://github.com/FMInference/H2O.},
  urldate      = {2024-11-23}
}

@ARTICLE{Hu-LoRA-Low-RankModels-2021,
  title        = {{LoRA}: Low-Rank Adaptation of large language models},
  author       = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and
                  Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu
                  and Chen, Weizhu},
  journaltitle = {arXiv [cs.CL]},
  date         = {2021-06-17},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {An important paradigm of natural language processing consists
                  of large-scale pre-training on general domain data and
                  adaptation to particular tasks or domains. As we pre-train
                  larger models, full fine-tuning, which retrains all model
                  parameters, becomes less feasible. Using GPT-3 175B as an
                  example -- deploying independent instances of fine-tuned
                  models, each with 175B parameters, is prohibitively expensive.
                  We propose Low-Rank Adaptation, or LoRA, which freezes the
                  pre-trained model weights and injects trainable rank
                  decomposition matrices into each layer of the Transformer
                  architecture, greatly reducing the number of trainable
                  parameters for downstream tasks. Compared to GPT-3 175B
                  fine-tuned with Adam, LoRA can reduce the number of trainable
                  parameters by 10,000 times and the GPU memory requirement by 3
                  times. LoRA performs on-par or better than fine-tuning in
                  model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite
                  having fewer trainable parameters, a higher training
                  throughput, and, unlike adapters, no additional inference
                  latency. We also provide an empirical investigation into
                  rank-deficiency in language model adaptation, which sheds
                  light on the efficacy of LoRA. We release a package that
                  facilitates the integration of LoRA with PyTorch models and
                  provide our implementations and model checkpoints for RoBERTa,
                  DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  urldate      = {2024-11-04}
}

@ARTICLE{Sheng-S-LoRA-ServingAdapters-2023,
  title        = {{S}-{LoRA}: Serving thousands of concurrent {LoRA} adapters},
  author       = {Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman
                  and Lee, Nicholas and Yang, Shuo and Chou, Christopher and
                  Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and
                  Gonzalez, Joseph E and Stoica, Ion},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-11-06},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {The "pretrain-then-finetune" paradigm is commonly adopted in
                  the deployment of large language models. Low-Rank Adaptation
                  (LoRA), a parameter-efficient fine-tuning method, is often
                  employed to adapt a base model to a multitude of tasks,
                  resulting in a substantial collection of LoRA adapters derived
                  from one base model. We observe that this paradigm presents
                  significant opportunities for batched inference during
                  serving. To capitalize on these opportunities, we present
                  S-LoRA, a system designed for the scalable serving of many
                  LoRA adapters. S-LoRA stores all adapters in the main memory
                  and fetches the adapters used by the currently running queries
                  to the GPU memory. To efficiently use the GPU memory and
                  reduce fragmentation, S-LoRA proposes Unified Paging. Unified
                  Paging uses a unified memory pool to manage dynamic adapter
                  weights with different ranks and KV cache tensors with varying
                  sequence lengths. Additionally, S-LoRA employs a novel tensor
                  parallelism strategy and highly optimized custom CUDA kernels
                  for heterogeneous batching of LoRA computation. Collectively,
                  these features enable S-LoRA to serve thousands of LoRA
                  adapters on a single GPU or across multiple GPUs with a small
                  overhead. Compared to state-of-the-art libraries such as
                  HuggingFace PEFT and vLLM (with naive support of LoRA
                  serving), S-LoRA can improve the throughput by up to 4 times
                  and increase the number of served adapters by several orders
                  of magnitude. As a result, S-LoRA enables scalable serving of
                  many task-specific fine-tuned models and offers the potential
                  for large-scale customized fine-tuning services. The code is
                  available at https://github.com/S-LoRA/S-LoRA},
  urldate      = {2024-11-04}
}

@ARTICLE{Nijkamp-ProGen2-ExploringModels-2022,
  title        = {{ProGen2}: Exploring the boundaries of protein language models},
  author       = {Nijkamp, Erik and Ruffolo, Jeffrey and Weinstein, Eli N and
                  Naik, Nikhil and Madani, Ali},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-06-27},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Attention-based models trained on protein sequences have
                  demonstrated incredible success at classification and
                  generation tasks relevant for artificial intelligence-driven
                  protein design. However, we lack a sufficient understanding of
                  how very large-scale models and data play a role in effective
                  protein model development. We introduce a suite of protein
                  language models, named ProGen2, that are scaled up to 6.4B
                  parameters and trained on different sequence datasets drawn
                  from over a billion proteins from genomic, metagenomic, and
                  immune repertoire databases. ProGen2 models show
                  state-of-the-art performance in capturing the distribution of
                  observed evolutionary sequences, generating novel viable
                  sequences, and predicting protein fitness without additional
                  finetuning. As large model sizes and raw numbers of protein
                  sequences continue to become more widely accessible, our
                  results suggest that a growing emphasis needs to be placed on
                  the data distribution provided to a protein sequence model. We
                  release the ProGen2 models and code at
                  https://github.com/salesforce/progen.},
  urldate      = {2024-11-11}
}

@ARTICLE{Zhilin-HotpotQA-DatasetAnswering-2018,
  title        = {{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop
                  Question Answering},
  author       = {Zhilin, Yang and Peng, Qi and Saizheng, Zhang and Yoshua,
                  Bengio and William, W Cohen and Ruslan, Salakhutdinov and
                  Christopher, D Manning},
  journaltitle = {arXiv [cs.CL]},
  date         = {2018-09-25},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Existing question answering (QA) datasets fail to train QA
                  systems to perform complex reasoning and provide explanations
                  for answers. We introduce HotpotQA, a new dataset with 113k
                  Wikipedia-based question-answer pairs with four key features:
                  (1) the questions require finding and reasoning over multiple
                  supporting documents to answer; (2) the questions are diverse
                  and not constrained to any pre-existing knowledge bases or
                  knowledge schemas; (3) we provide sentence-level supporting
                  facts required for reasoning, allowing QA systems to reason
                  with strong supervision and explain the predictions; (4) we
                  offer a new type of factoid comparison questions to test QA
                  systems' ability to extract relevant facts and perform
                  necessary comparison. We show that HotpotQA is challenging for
                  the latest QA systems, and the supporting facts enable models
                  to improve performance and make explainable predictions.},
  urldate      = {2024-09-29}
}

@ARTICLE{Zhou-WebArena-RealisticAgents-2023,
  title        = {{WebArena}: A realistic web environment for building
                  autonomous agents},
  author       = {Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and
                  Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou,
                  Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and
                  Neubig, Graham},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-07-25},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {With advances in generative AI, there is now potential for
                  autonomous agents to manage daily tasks via natural language
                  commands. However, current agents are primarily created and
                  tested in simplified synthetic environments, leading to a
                  disconnect with real-world scenarios. In this paper, we build
                  an environment for language-guided agents that is highly
                  realistic and reproducible. Specifically, we focus on agents
                  that perform tasks on the web, and create an environment with
                  fully functional websites from four common domains:
                  e-commerce, social forum discussions, collaborative software
                  development, and content management. Our environment is
                  enriched with tools (e.g., a map) and external knowledge bases
                  (e.g., user manuals) to encourage human-like task-solving.
                  Building upon our environment, we release a set of benchmark
                  tasks focusing on evaluating the functional correctness of
                  task completions. The tasks in our benchmark are diverse,
                  long-horizon, and designed to emulate tasks that humans
                  routinely perform on the internet. We experiment with several
                  baseline agents, integrating recent techniques such as
                  reasoning before acting. The results demonstrate that solving
                  complex tasks is challenging: our best GPT-4-based agent only
                  achieves an end-to-end task success rate of 14.41\%,
                  significantly lower than the human performance of 78.24\%.
                  These results highlight the need for further development of
                  robust agents, that current state-of-the-art large language
                  models are far from perfect performance in these real-life
                  tasks, and that WebArena can be used to measure such progress.},
  urldate      = {2024-09-29}
}

@ARTICLE{Mialon-GAIA-BenchmarkAssistants-2023,
  title        = {{GAIA}: a benchmark for General {AI} Assistants},
  author       = {Mialon, Grégoire and Fourrier, Clémentine and Swift, Craig and
                  Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-11-21},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {We introduce GAIA, a benchmark for General AI Assistants that,
                  if solved, would represent a milestone in AI research. GAIA
                  proposes real-world questions that require a set of
                  fundamental abilities such as reasoning, multi-modality
                  handling, web browsing, and generally tool-use proficiency.
                  GAIA questions are conceptually simple for humans yet
                  challenging for most advanced AIs: we show that human
                  respondents obtain 92\% vs. 15\% for GPT-4 equipped with
                  plugins. This notable performance disparity contrasts with the
                  recent trend of LLMs outperforming humans on tasks requiring
                  professional skills in e.g. law or chemistry. GAIA's
                  philosophy departs from the current trend in AI benchmarks
                  suggesting to target tasks that are ever more difficult for
                  humans. We posit that the advent of Artificial General
                  Intelligence (AGI) hinges on a system's capability to exhibit
                  similar robustness as the average human does on such
                  questions. Using GAIA's methodology, we devise 466 questions
                  and their answer. We release our questions while retaining
                  answers to 300 of them to power a leader-board available at
                  https://huggingface.co/gaia-benchmark.},
  urldate      = {2024-09-29}
}

@ARTICLE{Jimenez-SWE-bench-Language-2023,
  title        = {{SWE}-bench: Can language models resolve real-world {GitHub}
                  issues?},
  author       = {Jimenez, Carlos E and Yang, John and Wettig, Alexander and
                  Yao, Shunyu and Pei, Kexin and {Ofir Press} and Narasimhan,
                  Karthik},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-10},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Language models have outpaced our ability to evaluate them
                  effectively, but for their future development it is essential
                  to study the frontier of their capabilities. We find
                  real-world software engineering to be a rich, sustainable, and
                  challenging testbed for evaluating the next generation of
                  language models. To this end, we introduce SWE-bench, an
                  evaluation framework consisting of $2,294$ software
                  engineering problems drawn from real GitHub issues and
                  corresponding pull requests across $12$ popular Python
                  repositories. Given a codebase along with a description of an
                  issue to be resolved, a language model is tasked with editing
                  the codebase to address the issue. Resolving issues in
                  SWE-bench frequently requires understanding and coordinating
                  changes across multiple functions, classes, and even files
                  simultaneously, calling for models to interact with execution
                  environments, process extremely long contexts and perform
                  complex reasoning that goes far beyond traditional code
                  generation tasks. Our evaluations show that both
                  state-of-the-art proprietary models and our fine-tuned model
                  SWE-Llama can resolve only the simplest issues. The
                  best-performing model, Claude 2, is able to solve a mere
                  $1.96$\% of the issues. Advances on SWE-bench represent steps
                  towards LMs that are more practical, intelligent, and
                  autonomous.},
  urldate      = {2024-09-29}
}

@ARTICLE{Austin-Program-SynthesisModels-2021,
  title        = {Program synthesis with large language models},
  author       = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma,
                  Maarten and Michalewski, Henryk and Dohan, David and Jiang,
                  Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and
                  Sutton, Charles},
  journaltitle = {arXiv [cs.PL]},
  date         = {2021-08-15},
  eprinttype   = {arXiv},
  eprintclass  = {cs.PL},
  abstract     = {This paper explores the limits of the current generation of
                  large language models for program synthesis in general purpose
                  programming languages. We evaluate a collection of such models
                  (with between 244M and 137B parameters) on two new benchmarks,
                  MBPP and MathQA-Python, in both the few-shot and fine-tuning
                  regimes. Our benchmarks are designed to measure the ability of
                  these models to synthesize short Python programs from natural
                  language descriptions. The Mostly Basic Programming Problems
                  (MBPP) dataset contains 974 programming tasks, designed to be
                  solvable by entry-level programmers. The MathQA-Python
                  dataset, a Python version of the MathQA benchmark, contains
                  23914 problems that evaluate the ability of the models to
                  synthesize code from more complex text. On both datasets, we
                  find that synthesis performance scales log-linearly with model
                  size. Our largest models, even without finetuning on a code
                  dataset, can synthesize solutions to 59.6 percent of the
                  problems from MBPP using few-shot learning with a
                  well-designed prompt. Fine-tuning on a held-out portion of the
                  dataset improves performance by about 10 percentage points
                  across most model sizes. On the MathQA-Python dataset, the
                  largest fine-tuned model achieves 83.8 percent accuracy. Going
                  further, we study the model's ability to engage in dialog
                  about code, incorporating human feedback to improve its
                  solutions. We find that natural language feedback from a human
                  halves the error rate compared to the model's initial
                  prediction. Additionally, we conduct an error analysis to shed
                  light on where these models fall short and what types of
                  programs are most difficult to generate. Finally, we explore
                  the semantic grounding of these models by fine-tuning them to
                  predict the results of program execution. We find that even
                  our best models are generally unable to predict the output of
                  a program given a specific input.},
  urldate      = {2024-09-29}
}

@ARTICLE{Li-WebSuite-SystematicallyFail-2024,
  title        = {{WebSuite}: Systematically evaluating why web agents fail},
  author       = {Li, Eric and Waldo, Jim},
  journaltitle = {arXiv [cs.SE]},
  date         = {2024-05-31},
  eprinttype   = {arXiv},
  eprintclass  = {cs.SE},
  abstract     = {We describe WebSuite, the first diagnostic benchmark for
                  generalist web agents, designed to systematically evaluate why
                  agents fail. Advances in AI have led to the rise of numerous
                  web agents that autonomously operate a browser to complete
                  tasks. However, most existing benchmarks focus on strictly
                  measuring whether an agent can or cannot complete a task,
                  without giving insight on why. In this paper, we 1) develop a
                  taxonomy of web actions to facilitate identifying common
                  failure patterns, and 2) create an extensible benchmark suite
                  to assess agents' performance on our taxonomized actions. This
                  benchmark suite consists of both individual tasks, such as
                  clicking a button, and end-to-end tasks, such as adding an
                  item to a cart, and is designed such that any failure of a
                  task can be attributed directly to a failure of a specific web
                  action. We evaluate two popular generalist web agents, one
                  text-based and one multimodal, and identify unique weaknesses
                  for each agent. Because WebSuite can disaggregate task
                  failures into specific action failures, this enables granular
                  identification of which UX flows an individual agent has
                  trouble with and immediately highlights promising avenues for
                  improvement. These findings highlight the need for more
                  focused benchmarking on where web agents go wrong to
                  effectively improve agents beyond their weaker performance
                  today.},
  urldate      = {2024-09-29}
}

@ARTICLE{Rein-GPQA-Graduate-LevelBenchmark-2023,
  title        = {{GPQA}: A Graduate-Level Google-Proof {Q\&A} Benchmark},
  author       = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and
                  Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien
                  and Michael, Julian and Bowman, Samuel R},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-11-20},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {We present GPQA, a challenging dataset of 448 multiple-choice
                  questions written by domain experts in biology, physics, and
                  chemistry. We ensure that the questions are high-quality and
                  extremely difficult: experts who have or are pursuing PhDs in
                  the corresponding domains reach 65\% accuracy (74\% when
                  discounting clear mistakes the experts identified in
                  retrospect), while highly skilled non-expert validators only
                  reach 34\% accuracy, despite spending on average over 30
                  minutes with unrestricted access to the web (i.e., the
                  questions are "Google-proof"). The questions are also
                  difficult for state-of-the-art AI systems, with our strongest
                  GPT-4 based baseline achieving 39\% accuracy. If we are to use
                  future AI systems to help us answer very hard questions, for
                  example, when developing new scientific knowledge, we need to
                  develop scalable oversight methods that enable humans to
                  supervise their outputs, which may be difficult even if the
                  supervisors are themselves skilled and knowledgeable. The
                  difficulty of GPQA both for skilled non-experts and frontier
                  AI systems should enable realistic scalable oversight
                  experiments, which we hope can help devise ways for human
                  experts to reliably get truthful information from AI systems
                  that surpass human capabilities.}
}

@ARTICLE{Chen-Evaluating-LargeCode-2021,
  title        = {Evaluating large language models trained on code},
  author       = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming
                  and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and
                  Edwards, Harri and Burda, Yuri and Joseph, Nicholas and
                  Brockman, Greg and Ray, Alex and Puri, Raul and Krueger,
                  Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry,
                  Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott
                  and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and
                  Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and
                  Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave
                  and Plappert, Matthias and Chantzis, Fotios and Barnes,
                  Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and
                  Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie
                  and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and
                  Saunders, William and Hesse, Christopher and Carr, Andrew N
                  and Leike, Jan and Achiam, Josh and Misra, Vedant and
                  Morikawa, Evan and Radford, Alec and Knight, Matthew and
                  Brundage, Miles and Murati, Mira and Mayer, Katie and
                  Welinder, Peter and McGrew, Bob and Amodei, Dario and
                  McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  journaltitle = {arXiv [cs.LG]},
  date         = {2021-07-07},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {We introduce Codex, a GPT language model fine-tuned on
                  publicly available code from GitHub, and study its Python
                  code-writing capabilities. A distinct production version of
                  Codex powers GitHub Copilot. On HumanEval, a new evaluation
                  set we release to measure functional correctness for
                  synthesizing programs from docstrings, our model solves 28.8\%
                  of the problems, while GPT-3 solves 0\% and GPT-J solves
                  11.4\%. Furthermore, we find that repeated sampling from the
                  model is a surprisingly effective strategy for producing
                  working solutions to difficult prompts. Using this method, we
                  solve 70.2\% of our problems with 100 samples per problem.
                  Careful investigation of our model reveals its limitations,
                  including difficulty with docstrings describing long chains of
                  operations and with binding operations to variables. Finally,
                  we discuss the potential broader impacts of deploying powerful
                  code generation technologies, covering safety, security, and
                  economics.},
  urldate      = {2024-09-29}
}

@ARTICLE{Wang-MINT-EvaluatingFeedback-2023,
  title        = {{MINT}: Evaluating {LLMs} in multi-turn interaction with tools
                  and language feedback},
  author       = {Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen,
                  Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-09-19},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {To solve complex tasks, large language models (LLMs) often
                  require multiple rounds of interactions with the user,
                  sometimes assisted by external tools. However, current
                  evaluation protocols often emphasize benchmark performance
                  with single-turn exchanges, neglecting the nuanced
                  interactions among the user, LLMs, and external tools, while
                  also underestimating the importance of natural language
                  feedback from users. These oversights contribute to
                  discrepancies between research benchmark evaluations and
                  real-world use cases. We introduce MINT, a benchmark that
                  evaluates LLMs' ability to solve tasks with multi-turn
                  interactions by (1) using tools and (2) leveraging natural
                  language feedback. To ensure reproducibility, we provide an
                  evaluation framework where LLMs can access tools by executing
                  Python code and receive users' natural language feedback
                  simulated by GPT-4. We repurpose a diverse set of established
                  evaluation datasets focusing on reasoning, coding, and
                  decision-making and carefully curate them into a compact
                  subset for efficient evaluation. Our analysis of 20 open- and
                  closed-source LLMs offers intriguing findings. (a) LLMs
                  generally benefit from tools and language feedback, with
                  performance gains (absolute, same below) of 1-8\% for each
                  turn of tool use and 2-17\% with natural language feedback.
                  (b) Better single-turn performance does not guarantee better
                  multi-turn performance. (c) Surprisingly, on the LLMs
                  evaluated, supervised instruction-finetuning (SIFT) and
                  reinforcement learning from human feedback (RLHF) generally
                  hurt multi-turn capabilities. We expect MINT can help measure
                  progress and incentivize research in improving LLMs'
                  capabilities in multi-turn interactions, especially for
                  open-source communities where multi-turn human evaluation can
                  be less accessible compared to commercial LLMs with a larger
                  user base.},
  urldate      = {2024-09-29}
}

@ARTICLE{Kapoor-AI-Agents-Matter-2024,
  title        = {{AI} Agents That Matter},
  author       = {Kapoor, Sayash and Stroebl, Benedikt and Siegel, Zachary S and
                  Nadgir, Nitya and Narayanan, Arvind},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-07-01},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {AI agents are an exciting new research direction, and agent
                  development is driven by benchmarks. Our analysis of current
                  agent benchmarks and evaluation practices reveals several
                  shortcomings that hinder their usefulness in real-world
                  applications. First, there is a narrow focus on accuracy
                  without attention to other metrics. As a result, SOTA agents
                  are needlessly complex and costly, and the community has
                  reached mistaken conclusions about the sources of accuracy
                  gains. Our focus on cost in addition to accuracy motivates the
                  new goal of jointly optimizing the two metrics. We design and
                  implement one such optimization, showing its potential to
                  greatly reduce cost while maintaining accuracy. Second, the
                  benchmarking needs of model and downstream developers have
                  been conflated, making it hard to identify which agent would
                  be best suited for a particular application. Third, many agent
                  benchmarks have inadequate holdout sets, and sometimes none at
                  all. This has led to agents that are fragile because they take
                  shortcuts and overfit to the benchmark in various ways. We
                  prescribe a principled framework for avoiding overfitting.
                  Finally, there is a lack of standardization in evaluation
                  practices, leading to a pervasive lack of reproducibility. We
                  hope that the steps we introduce for addressing these
                  shortcomings will spur the development of agents that are
                  useful in the real world and not just accurate on benchmarks.},
  urldate      = {2024-09-28}
}

@ARTICLE{Wallace-Instruction-HierarchyInstructions-2024,
  title        = {The instruction hierarchy: Training {LLMs} to prioritize
                  privileged instructions},
  author       = {Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian
                  and Heidecke, Johannes and Beutel, Alex},
  journaltitle = {arXiv [cs.CR]},
  date         = {2024-04-19},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CR},
  abstract     = {Today's LLMs are susceptible to prompt injections, jailbreaks,
                  and other attacks that allow adversaries to overwrite a
                  model's original instructions with their own malicious
                  prompts. In this work, we argue that one of the primary
                  vulnerabilities underlying these attacks is that LLMs often
                  consider system prompts (e.g., text from an application
                  developer) to be the same priority as text from untrusted
                  users and third parties. To address this, we propose an
                  instruction hierarchy that explicitly defines how models
                  should behave when instructions of different priorities
                  conflict. We then propose a data generation method to
                  demonstrate this hierarchical instruction following behavior,
                  which teaches LLMs to selectively ignore lower-privileged
                  instructions. We apply this method to GPT-3.5, showing that it
                  drastically increases robustness -- even for attack types not
                  seen during training -- while imposing minimal degradations on
                  standard capabilities.},
  urldate      = {2024-09-30}
}

@ARTICLE{Mu-LLMs-Follow-2023,
  title        = {Can {LLMs} Follow Simple Rules?},
  author       = {Mu, Norman and Chen, Sarah and Wang, Zifan and Chen, Sizhe and
                  Karamardian, David and Aljeraisy, Lulwa and Alomair, Basel and
                  Hendrycks, Dan and Wagner, David},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-11-06},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {As Large Language Models (LLMs) are deployed with increasing
                  real-world responsibilities, it is important to be able to
                  specify and constrain the behavior of these systems in a
                  reliable manner. Model developers may wish to set explicit
                  rules for the model, such as "do not generate abusive
                  content", but these may be circumvented by jailbreaking
                  techniques. Existing evaluations of adversarial attacks and
                  defenses on LLMs generally require either expensive manual
                  review or unreliable heuristic checks. To address this issue,
                  we propose Rule-following Language Evaluation Scenarios
                  (RuLES), a programmatic framework for measuring rule-following
                  ability in LLMs. RuLES consists of 14 simple text scenarios in
                  which the model is instructed to obey various rules while
                  interacting with the user. Each scenario has a programmatic
                  evaluation function to determine whether the model has broken
                  any rules in a conversation. Our evaluations of proprietary
                  and open models show that almost all current models struggle
                  to follow scenario rules, even on straightforward test cases.
                  We also demonstrate that simple optimization attacks suffice
                  to significantly increase failure rates on test cases. We
                  conclude by exploring two potential avenues for improvement:
                  test-time steering and supervised fine-tuning.},
  urldate      = {2024-09-30}
}

@INPROCEEDINGS{Ansel-PyTorch-2Compilation-2024,
  title      = {{PyTorch} 2: Faster machine learning through dynamic python
                bytecode transformation and graph compilation},
  author     = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein,
                Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin
                and Bell, Peter and Berard, David and Burovski, Evgeni and
                Chauhan, Geeta and Chourdia, Anjali and Constable, Will and
                Desmaison, Alban and DeVito, Zachary and Ellison, Elias and
                Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh,
                Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch,
                Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo
                and Liang, Jason and Lu, Yinghai and Luk, C K and Maher, Bert
                and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and
                Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and
                Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu
                and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang,
                Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory
                and Wu, Peng and Chintala, Soumith},
  booktitle  = {Proceedings of the 29th ACM International Conference on
                Architectural Support for Programming Languages and Operating
                Systems, Volume 2},
  publisher  = {ACM},
  location   = {New York, NY, USA},
  eventtitle = {ASPLOS '24: 29th ACM International Conference on Architectural
                Support for Programming Languages and Operating Systems, Volume
                2},
  venue      = {La Jolla CA USA},
  date       = {2024-04-27},
  urldate    = {2024-10-07},
  language   = {en}
}

@ARTICLE{Wu-Instructional-SegmentHierarchy-2024,
  title        = {Instructional Segment Embedding: Improving {LLM} safety with
                  Instruction Hierarchy},
  author       = {Wu, Tong and Zhang, Shujian and Song, Kaiqiang and Xu, Silei
                  and Zhao, Sanqiang and Agrawal, Ravi and Indurthi, Sathish
                  Reddy and Xiang, Chong and Mittal, Prateek and Zhou, Wenxuan},
  journaltitle = {arXiv [cs.CR]},
  date         = {2024-10-09},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CR},
  abstract     = {Large Language Models (LLMs) are susceptible to security and
                  safety threats, such as prompt injection, prompt extraction,
                  and harmful requests. One major cause of these vulnerabilities
                  is the lack of an instruction hierarchy. Modern LLM
                  architectures treat all inputs equally, failing to distinguish
                  between and prioritize various types of instructions, such as
                  system messages, user prompts, and data. As a result,
                  lower-priority user prompts may override more critical system
                  instructions, including safety protocols. Existing approaches
                  to achieving instruction hierarchy, such as delimiters and
                  instruction-based training, do not address this issue at the
                  architectural level. We introduce the Instructional Segment
                  Embedding (ISE) technique, inspired by BERT, to modern large
                  language models, which embeds instruction priority information
                  directly into the model. This approach enables models to
                  explicitly differentiate and prioritize various instruction
                  types, significantly improving safety against malicious
                  prompts that attempt to override priority rules. Our
                  experiments on the Structured Query and Instruction Hierarchy
                  benchmarks demonstrate an average robust accuracy increase of
                  up to 15.75\% and 18.68\%, respectively. Furthermore, we
                  observe an improvement in instruction-following capability of
                  up to 4.1\% evaluated on AlpacaEval. Overall, our approach
                  offers a promising direction for enhancing the safety and
                  effectiveness of LLM architectures.},
  urldate      = {2024-10-22}
}

@ARTICLE{Shoeybi-Megatron-LM-TrainingParallelism-2019,
  title        = {Megatron-{LM}: Training multi-billion parameter language
                  models using model parallelism},
  author       = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and
                  LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journaltitle = {arXiv [cs.CL]},
  date         = {2019-09-17},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Recent work in language modeling demonstrates that training
                  large transformer models advances the state of the art in
                  Natural Language Processing applications. However, very large
                  models can be quite difficult to train due to memory
                  constraints. In this work, we present our techniques for
                  training very large transformer models and implement a simple,
                  efficient intra-layer model parallel approach that enables
                  training transformer models with billions of parameters. Our
                  approach does not require a new compiler or library changes,
                  is orthogonal and complimentary to pipeline model parallelism,
                  and can be fully implemented with the insertion of a few
                  communication operations in native PyTorch. We illustrate this
                  approach by converging transformer based models up to 8.3
                  billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs
                  across the entire application with 76\% scaling efficiency
                  when compared to a strong single GPU baseline that sustains 39
                  TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that
                  large language models can further advance the state of the art
                  (SOTA), we train an 8.3 billion parameter transformer language
                  model similar to GPT-2 and a 3.9 billion parameter model
                  similar to BERT. We show that careful attention to the
                  placement of layer normalization in BERT-like models is
                  critical to achieving increased performance as the model size
                  grows. Using the GPT-2 model we achieve SOTA results on the
                  WikiText103 (10.8 compared to SOTA perplexity of 15.8) and
                  LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets.
                  Our BERT model achieves SOTA results on the RACE dataset
                  (90.9\% compared to SOTA accuracy of 89.4\%).},
  urldate      = {2024-10-21}
}

@ARTICLE{Zou-Representation-EngineeringTransparency-2023,
  title        = {Representation engineering: A top-down approach to {AI}
                  transparency},
  author       = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James
                  and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin,
                  Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and
                  Goel, Shashwat and Li, Nathaniel and Byun, Michael J and Wang,
                  Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi
                  and Song, Dawn and Fredrikson, Matt and Kolter, J Zico and
                  Hendrycks, Dan},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-10-02},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {In this paper, we identify and characterize the emerging area
                  of representation engineering (RepE), an approach to enhancing
                  the transparency of AI systems that draws on insights from
                  cognitive neuroscience. RepE places population-level
                  representations, rather than neurons or circuits, at the
                  center of analysis, equipping us with novel methods for
                  monitoring and manipulating high-level cognitive phenomena in
                  deep neural networks (DNNs). We provide baselines and an
                  initial analysis of RepE techniques, showing that they offer
                  simple yet effective solutions for improving our understanding
                  and control of large language models. We showcase how these
                  methods can provide traction on a wide range of
                  safety-relevant problems, including honesty, harmlessness,
                  power-seeking, and more, demonstrating the promise of top-down
                  transparency research. We hope that this work catalyzes
                  further exploration of RepE and fosters advancements in the
                  transparency and safety of AI systems.},
  urldate      = {2024-10-30}
}

@ARTICLE{Dao-FlashAttention-Memory-efficientIO-awareness-2022,
  title        = {{FlashAttention}: Fast and memory-efficient exact attention
                  with {IO}-awareness},
  author       = {Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri
                  and Ré, Christopher},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-05-27},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Transformers are slow and memory-hungry on long sequences,
                  since the time and memory complexity of self-attention are
                  quadratic in sequence length. Approximate attention methods
                  have attempted to address this problem by trading off model
                  quality to reduce the compute complexity, but often do not
                  achieve wall-clock speedup. We argue that a missing principle
                  is making attention algorithms IO-aware -- accounting for
                  reads and writes between levels of GPU memory. We propose
                  FlashAttention, an IO-aware exact attention algorithm that
                  uses tiling to reduce the number of memory reads/writes
                  between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.
                  We analyze the IO complexity of FlashAttention, showing that
                  it requires fewer HBM accesses than standard attention, and is
                  optimal for a range of SRAM sizes. We also extend
                  FlashAttention to block-sparse attention, yielding an
                  approximate attention algorithm that is faster than any
                  existing approximate attention method. FlashAttention trains
                  Transformers faster than existing baselines: 15\% end-to-end
                  wall-clock speedup on BERT-large (seq. length 512) compared to
                  the MLPerf 1.1 training speed record, 3$\times$ speedup on
                  GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range
                  arena (seq. length 1K-4K). FlashAttention and block-sparse
                  FlashAttention enable longer context in Transformers, yielding
                  higher quality models (0.7 better perplexity on GPT-2 and 6.4
                  points of lift on long-document classification) and entirely
                  new capabilities: the first Transformers to achieve
                  better-than-chance performance on the Path-X challenge (seq.
                  length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K,
                  63.1\% accuracy).},
  urldate      = {2024-10-19}
}

@ARTICLE{Zheng-Alpa-AutomatingLearning-2022,
  title        = {Alpa: Automating inter- and intra-operator parallelism for
                  distributed deep learning},
  author       = {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang,
                  Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida
                  and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and
                  Gonzalez, Joseph E and Stoica, Ion},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-01-28},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Alpa automates model-parallel training of large deep learning
                  (DL) models by generating execution plans that unify data,
                  operator, and pipeline parallelism. Existing model-parallel
                  training systems either require users to manually create a
                  parallelization plan or automatically generate one from a
                  limited space of model parallelism configurations. They do not
                  suffice to scale out complex DL models on distributed compute
                  devices. Alpa distributes the training of large DL models by
                  viewing parallelisms as two hierarchical levels:
                  inter-operator and intra-operator parallelisms. Based on it,
                  Alpa constructs a new hierarchical space for massive
                  model-parallel execution plans. Alpa designs a number of
                  compilation passes to automatically derive efficient parallel
                  execution plans at each parallelism level. Alpa implements an
                  efficient runtime to orchestrate the two-level parallel
                  execution on distributed compute devices. Our evaluation shows
                  Alpa generates parallelization plans that match or outperform
                  hand-tuned model-parallel training systems even on models they
                  are designed for. Unlike specialized systems, Alpa also
                  generalizes to models with heterogeneous architectures and
                  models without manually-designed plans. Alpa's source code is
                  publicly available at https://github.com/alpa-projects/alpa},
  urldate      = {2024-10-21}
}

@ARTICLE{Liu-Ring-AttentionContext-2023,
  title        = {Ring Attention with Blockwise Transformers for near-infinite
                  context},
  author       = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-03},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Transformers have emerged as the architecture of choice for
                  many state-of-the-art AI models, showcasing exceptional
                  performance across a wide range of AI applications. However,
                  the memory demands imposed by Transformers limit their ability
                  to handle long sequences, thereby posing challenges in
                  utilizing videos, actions, and other long-form sequences and
                  modalities in complex environments. We present a novel
                  approach, Ring Attention with Blockwise Transformers (Ring
                  Attention), which leverages blockwise computation of
                  self-attention and feedforward to distribute long sequences
                  across multiple devices while fully overlapping the
                  communication of key-value blocks with the computation of
                  blockwise attention. Our approach enables training and
                  inference of sequences that are up to device count times
                  longer than those achievable by prior memory-efficient
                  Transformers, without resorting to approximations or incurring
                  additional communication and computation overheads. Extensive
                  experiments on language modeling and reinforcement learning
                  tasks demonstrate the effectiveness of our approach in
                  allowing millions of tokens context size and improving
                  performance.},
  urldate      = {2024-10-19}
}

@ARTICLE{Jouppi-TPU-V4Embeddings-2023,
  title        = {{TPU} {v4}: An optically reconfigurable supercomputer for
                  machine learning with hardware support for embeddings},
  author       = {Jouppi, Norman P and Kurian, George and Li, Sheng and Ma,
                  Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant
                  and Subramanian, Suvinay and Swing, Andy and Towles, Brian and
                  Young, Cliff and Zhou, Xiang and Zhou, Zongwei and Patterson,
                  David},
  journaltitle = {arXiv [cs.AR]},
  date         = {2023-04-03},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AR},
  abstract     = {In response to innovations in machine learning (ML) models,
                  production workloads changed radically and rapidly. TPU v4 is
                  the fifth Google domain specific architecture (DSA) and its
                  third supercomputer for such ML models. Optical circuit
                  switches (OCSes) dynamically reconfigure its interconnect
                  topology to improve scale, availability, utilization,
                  modularity, deployment, security, power, and performance;
                  users can pick a twisted 3D torus topology if desired. Much
                  cheaper, lower power, and faster than Infiniband, OCSes and
                  underlying optical components are <5\% of system cost and <3\%
                  of system power. Each TPU v4 includes SparseCores, dataflow
                  processors that accelerate models that rely on embeddings by
                  5x-7x yet use only 5\% of die area and power. Deployed since
                  2020, TPU v4 outperforms TPU v3 by 2.1x and improves
                  performance/Watt by 2.7x. The TPU v4 supercomputer is 4x
                  larger at 4096 chips and thus ~10x faster overall, which along
                  with OCS flexibility helps large language models. For similar
                  sized systems, it is ~4.3x-4.5x faster than the Graphcore IPU
                  Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than
                  the Nvidia A100. TPU v4s inside the energy-optimized warehouse
                  scale computers of Google Cloud use ~3x less energy and
                  produce ~20x less CO2e than contemporary DSAs in a typical
                  on-premise data center.},
  urldate      = {2024-10-07}
}

@ARTICLE{Penedo-FineWeb-DatasetsScale-2024,
  title        = {The {FineWeb} datasets: Decanting the web for the finest text
                  data at scale},
  author       = {Penedo, Guilherme and Kydlíček, Hynek and Allal, Loubna Ben
                  and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin
                  and Von Werra, Leandro and Wolf, Thomas},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-06-25},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {The performance of a large language model (LLM) depends
                  heavily on the quality and size of its pretraining dataset.
                  However, the pretraining datasets for state-of-the-art open
                  LLMs like Llama 3 and Mixtral are not publicly available and
                  very little is known about how they were created. In this
                  work, we introduce FineWeb, a 15-trillion token dataset
                  derived from 96 Common Crawl snapshots that produces
                  better-performing LLMs than other open pretraining datasets.
                  To advance the understanding of how best to curate
                  high-quality pretraining datasets, we carefully document and
                  ablate all of the design choices used in FineWeb, including
                  in-depth investigations of deduplication and filtering
                  strategies. In addition, we introduce FineWeb-Edu, a
                  1.3-trillion token collection of educational text filtered
                  from FineWeb. LLMs pretrained on FineWeb-Edu exhibit
                  dramatically better performance on knowledge- and
                  reasoning-intensive benchmarks like MMLU and ARC. Along with
                  our datasets, we publicly release our data curation codebase
                  and all of the models trained during our ablation experiments.},
  urldate      = {2024-10-12}
}

@ARTICLE{Soldaini-Dolma-OpenResearch-2024,
  title        = {Dolma: An open corpus of three trillion tokens for language
                  model pretraining research},
  author       = {Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and
                  Schwenk, Dustin and Atkinson, David and Authur, Russell and
                  Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar,
                  Yanai and Hofmann, Valentin and Jha, Ananya Harsh and Kumar,
                  Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and
                  Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and
                  Naik, Aakanksha and Nam, Crystal and Peters, Matthew E and
                  Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang
                  and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind
                  and Walsh, Pete and Zettlemoyer, Luke and Smith, Noah A and
                  Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and
                  Dodge, Jesse and Lo, Kyle},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-01-31},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Information about pretraining corpora used to train the
                  current best-performing language models is seldom discussed:
                  commercial models rarely detail their data, and even open
                  models are often released without accompanying training data
                  or recipes to reproduce them. As a result, it is challenging
                  to conduct and advance scientific research on language
                  modeling, such as understanding how training data impacts
                  model capabilities and limitations. To facilitate scientific
                  research on language model pretraining, we curate and release
                  Dolma, a three-trillion-token English corpus, built from a
                  diverse mixture of web content, scientific papers, code,
                  public-domain books, social media, and encyclopedic materials.
                  We extensively document Dolma, including its design
                  principles, details about its construction, and a summary of
                  its contents. We present analyses and experimental results on
                  intermediate states of Dolma to share what we have learned
                  about important data curation practices. Finally, we
                  open-source our data curation toolkit to enable reproduction
                  of our work as well as support further research in large-scale
                  data curation.},
  urldate      = {2024-10-12}
}

@ARTICLE{Chen-StruQ-DefendingQueries-2024,
  title        = {{StruQ}: Defending against prompt injection with structured
                  queries},
  author       = {Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner,
                  David},
  journaltitle = {arXiv [cs.CR]},
  date         = {2024-02-09},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CR},
  abstract     = {Recent advances in Large Language Models (LLMs) enable
                  exciting LLM-integrated applications, which perform text-based
                  tasks by utilizing their advanced language understanding
                  capabilities. However, as LLMs have improved, so have the
                  attacks against them. Prompt injection attacks are an
                  important threat: they trick the model into deviating from the
                  original application's instructions and instead follow user
                  directives. These attacks rely on the LLM's ability to follow
                  instructions and inability to separate prompts and user data.
                  We introduce structured queries, a general approach to tackle
                  this problem. Structured queries separate prompts and data
                  into two channels. We implement a system that supports
                  structured queries. This system is made of (1) a secure
                  front-end that formats a prompt and user data into a special
                  format, and (2) a specially trained LLM that can produce
                  high-quality outputs from these inputs. The LLM is trained
                  using a novel fine-tuning strategy: we convert a base
                  (non-instruction-tuned) LLM to a structured instruction-tuned
                  model that will only follow instructions in the prompt portion
                  of a query. To do so, we augment standard instruction tuning
                  datasets with examples that also include instructions in the
                  data portion of the query, and fine-tune the model to ignore
                  these. Our system significantly improves resistance to prompt
                  injection attacks, with little or no impact on utility. Our
                  code is released at https://github.com/Sizhe-Chen/StruQ.},
  urldate      = {2024-10-22}
}

@ARTICLE{Zhou-HAICOSYSTEM-EcosystemInteractions-2024,
  title        = {{HAICOSYSTEM}: An ecosystem for sandboxing safety risks in
                  human-{AI} interactions},
  author       = {Zhou, Xuhui and Kim, Hyunwoo and Brahman, Faeze and Jiang,
                  Liwei and Zhu, Hao and Lu, Ximing and Xu, Frank and Lin, Bill
                  Yuchen and Choi, Yejin and Mireshghallah, Niloofar and Bras,
                  Ronan Le and Sap, Maarten},
  journaltitle = {arXiv [cs.AI]},
  date         = {2024-09-24},
  eprinttype   = {arXiv},
  eprintclass  = {cs.AI},
  abstract     = {AI agents are increasingly autonomous in their interactions
                  with human users and tools, leading to increased interactional
                  safety risks. We present HAICOSYSTEM, a framework examining AI
                  agent safety within diverse and complex social interactions.
                  HAICOSYSTEM features a modular sandbox environment that
                  simulates multi-turn interactions between human users and AI
                  agents, where the AI agents are equipped with a variety of
                  tools (e.g., patient management platforms) to navigate diverse
                  scenarios (e.g., a user attempting to access other patients'
                  profiles). To examine the safety of AI agents in these
                  interactions, we develop a comprehensive multi-dimensional
                  evaluation framework that uses metrics covering operational,
                  content-related, societal, and legal risks. Through running
                  1840 simulations based on 92 scenarios across seven domains
                  (e.g., healthcare, finance, education), we demonstrate that
                  HAICOSYSTEM can emulate realistic user-AI interactions and
                  complex tool use by AI agents. Our experiments show that
                  state-of-the-art LLMs, both proprietary and open-sourced,
                  exhibit safety risks in over 50\% cases, with models generally
                  showing higher risks when interacting with simulated malicious
                  users. Our findings highlight the ongoing challenge of
                  building agents that can safely navigate complex interactions,
                  particularly when faced with malicious users. To foster the AI
                  agent safety ecosystem, we release a code platform that allows
                  practitioners to create custom scenarios, simulate
                  interactions, and evaluate the safety and performance of their
                  agents.},
  urldate      = {2024-10-21}
}

@ARTICLE{Andriushchenko-AgentHarm-BenchmarkAgents-2024,
  title        = {{AgentHarm}: A benchmark for measuring harmfulness of {LLM}
                  agents},
  author       = {Andriushchenko, Maksym and Souly, Alexandra and Dziemian,
                  Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin
                  and Hendrycks, Dan and Zou, Andy and Kolter, Zico and
                  Fredrikson, Matt and Winsor, Eric and Wynne, Jerome and Gal,
                  Yarin and Davies, Xander},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-10-11},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {The robustness of LLMs to jailbreak attacks, where users
                  design prompts to circumvent safety measures and misuse model
                  capabilities, has been studied primarily for LLMs acting as
                  simple chatbots. Meanwhile, LLM agents -- which use external
                  tools and can execute multi-stage tasks -- may pose a greater
                  risk if misused, but their robustness remains underexplored.
                  To facilitate research on LLM agent misuse, we propose a new
                  benchmark called AgentHarm. The benchmark includes a diverse
                  set of 110 explicitly malicious agent tasks (440 with
                  augmentations), covering 11 harm categories including fraud,
                  cybercrime, and harassment. In addition to measuring whether
                  models refuse harmful agentic requests, scoring well on
                  AgentHarm requires jailbroken agents to maintain their
                  capabilities following an attack to complete a multi-step
                  task. We evaluate a range of leading LLMs, and find (1)
                  leading LLMs are surprisingly compliant with malicious agent
                  requests without jailbreaking, (2) simple universal jailbreak
                  templates can be adapted to effectively jailbreak agents, and
                  (3) these jailbreaks enable coherent and malicious multi-step
                  agent behavior and retain model capabilities. To enable simple
                  and reliable evaluation of attacks and defenses for LLM-based
                  agents, we publicly release AgentHarm at
                  https://huggingface.co/datasets/ai-safety-institute/AgentHarm.},
  urldate      = {2024-10-22}
}

@ARTICLE{Agarwal-PerSim-Data-EfficientSimulators-2021,
  title        = {{PerSim}: Data-Efficient Offline Reinforcement Learning with
                  Heterogeneous Agents via Personalized Simulators},
  author       = {Agarwal, Anish and Alomar, Abdullah and Alumootil, Varkey and
                  Shah, Devavrat and Shen, Dennis and Xu, Zhi and Yang, Cindy},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume       = {34},
  pages        = {18564--18576},
  date         = {2021-12-06},
  urldate      = {2024-11-01}
}

@ARTICLE{Rajbhandari-ZeRO-MemoryModels-2019,
  title        = {{ZeRO}: Memory optimizations toward training Trillion
                  parameter models},
  author       = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and
                  He, Yuxiong},
  journaltitle = {arXiv [cs.LG]},
  date         = {2019-10-04},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Large deep learning models offer significant accuracy gains,
                  but training billions to trillions of parameters is
                  challenging. Existing solutions such as data and model
                  parallelisms exhibit fundamental limitations to fit these
                  models into limited device memory, while obtaining
                  computation, communication and development efficiency. We
                  develop a novel solution, Zero Redundancy Optimizer (ZeRO), to
                  optimize memory, vastly improving training speed while
                  increasing the model size that can be efficiently trained.
                  ZeRO eliminates memory redundancies in data- and
                  model-parallel training while retaining low communication
                  volume and high computational granularity, allowing us to
                  scale the model size proportional to the number of devices
                  with sustained high efficiency. Our analysis on memory
                  requirements and communication volume demonstrates: ZeRO has
                  the potential to scale beyond 1 Trillion parameters using
                  today's hardware. We implement and evaluate ZeRO: it trains
                  large models of over 100B parameter with super-linear speedup
                  on 400 GPUs, achieving throughput of 15 Petaflops. This
                  represents an 8x increase in model size and 10x increase in
                  achievable performance over state-of-the-art. In terms of
                  usability, ZeRO can train large models of up to 13B parameters
                  (e.g., larger than Megatron GPT 8.3B and T5 11B) without
                  requiring model parallelism which is harder for scientists to
                  apply. Last but not the least, researchers have used the
                  system breakthroughs of ZeRO to create the world's largest
                  language model (Turing-NLG, 17B parameters) with record
                  breaking accuracy.},
  urldate      = {2024-10-24}
}

@INPROCEEDINGS{Narayanan-PipeDream-GeneralizedTraining-2019,
  title      = {{PipeDream}: generalized pipeline parallelism for {DNN} training},
  author     = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and
                Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and
                Gibbons, Phillip B and Zaharia, Matei},
  booktitle  = {Proceedings of the 27th ACM Symposium on Operating Systems
                Principles},
  publisher  = {ACM},
  location   = {New York, NY, USA},
  eventtitle = {SOSP '19: ACM SIGOPS 27th Symposium on Operating Systems
                Principles},
  venue      = {Huntsville Ontario Canada},
  date       = {2019-10-27},
  urldate    = {2024-10-24},
  language   = {en}
}

@ARTICLE{Kumar2024-pc,
  title        = {Refusal-trained {LLMs} are easily jailbroken as browser agents},
  author       = {Kumar, Priyanshu and Lau, Elaine and Vijayakumar, Saranya and
                  Trinh, Tu and {Scale Red Team} and Chang, Elaine and Robinson,
                  Vaughn and Hendryx, Sean and Zhou, Shuyan and Fredrikson, Matt
                  and Yue, Summer and Wang, Zifan},
  journaltitle = {arXiv [cs.CR]},
  date         = {2024-10-11},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CR},
  abstract     = {For safety reasons, large language models (LLMs) are trained
                  to refuse harmful user instructions, such as assisting
                  dangerous activities. We study an open question in this work:
                  does the desired safety refusal, typically enforced in chat
                  contexts, generalize to non-chat and agentic use cases? Unlike
                  chatbots, LLM agents equipped with general-purpose tools, such
                  as web browsers and mobile devices, can directly influence the
                  real world, making it even more crucial to refuse harmful
                  instructions. In this work, we primarily focus on red-teaming
                  browser agents, LLMs that manipulate information via web
                  browsers. To this end, we introduce Browser Agent Red teaming
                  Toolkit (BrowserART), a comprehensive test suite designed
                  specifically for red-teaming browser agents. BrowserART is
                  consist of 100 diverse browser-related harmful behaviors
                  (including original behaviors and ones sourced from HarmBench
                  [Mazeika et al., 2024] and AirBench 2024 [Zeng et al., 2024b])
                  across both synthetic and real websites. Our empirical study
                  on state-of-the-art browser agents reveals that, while the
                  backbone LLM refuses harmful instructions as a chatbot, the
                  corresponding agent does not. Moreover, attack methods
                  designed to jailbreak refusal-trained LLMs in the chat
                  settings transfer effectively to browser agents. With human
                  rewrites, GPT-4o and o1-preview-based browser agents attempted
                  98 and 63 harmful behaviors (out of 100), respectively. We
                  publicly release BrowserART and call on LLM developers,
                  policymakers, and agent developers to collaborate on improving
                  agent safety}
}

@ARTICLE{Lightman-Lets-VerifyStep-2023,
  title        = {Let's verify step by step},
  author       = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and
                  Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan
                  and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-05-31},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {In recent years, large language models have greatly improved
                  in their ability to perform complex multi-step reasoning.
                  However, even state-of-the-art models still regularly produce
                  logical mistakes. To train more reliable models, we can turn
                  either to outcome supervision, which provides feedback for a
                  final result, or process supervision, which provides feedback
                  for each intermediate reasoning step. Given the importance of
                  training reliable models, and given the high cost of human
                  feedback, it is important to carefully compare the both
                  methods. Recent work has already begun this comparison, but
                  many questions still remain. We conduct our own investigation,
                  finding that process supervision significantly outperforms
                  outcome supervision for training models to solve problems from
                  the challenging MATH dataset. Our process-supervised model
                  solves 78\% of problems from a representative subset of the
                  MATH test set. Additionally, we show that active learning
                  significantly improves the efficacy of process supervision. To
                  support related research, we also release PRM800K, the
                  complete dataset of 800,000 step-level human feedback labels
                  used to train our best reward model.},
  urldate      = {2024-10-28}
}

@ARTICLE{Snell-Scaling-LLMParameters-2024,
  title        = {Scaling {LLM} test-time compute optimally can be more
                  effective than scaling model parameters},
  author       = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar,
                  Aviral},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-08-06},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {Enabling LLMs to improve their outputs by using more test-time
                  computation is a critical step towards building generally
                  self-improving agents that can operate on open-ended natural
                  language. In this paper, we study the scaling of
                  inference-time computation in LLMs, with a focus on answering
                  the question: if an LLM is allowed to use a fixed but
                  non-trivial amount of inference-time compute, how much can it
                  improve its performance on a challenging prompt? Answering
                  this question has implications not only on the achievable
                  performance of LLMs, but also on the future of LLM pretraining
                  and how one should tradeoff inference-time and pre-training
                  compute. Despite its importance, little research attempted to
                  understand the scaling behaviors of various test-time
                  inference methods. Moreover, current work largely provides
                  negative results for a number of these strategies. In this
                  work, we analyze two primary mechanisms to scale test-time
                  computation: (1) searching against dense, process-based
                  verifier reward models; and (2) updating the model's
                  distribution over a response adaptively, given the prompt at
                  test time. We find that in both cases, the effectiveness of
                  different approaches to scaling test-time compute critically
                  varies depending on the difficulty of the prompt. This
                  observation motivates applying a "compute-optimal" scaling
                  strategy, which acts to most effectively allocate test-time
                  compute adaptively per prompt. Using this compute-optimal
                  strategy, we can improve the efficiency of test-time compute
                  scaling by more than 4x compared to a best-of-N baseline.
                  Additionally, in a FLOPs-matched evaluation, we find that on
                  problems where a smaller base model attains somewhat
                  non-trivial success rates, test-time compute can be used to
                  outperform a 14x larger model.},
  urldate      = {2024-10-28}
}

@ARTICLE{Ouyang-Training-LanguageFeedback-2022,
  title        = {Training language models to follow instructions with human
                  feedback},
  author       = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and
                  Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and
                  Agarwal, Sandhini and Slama, Katarina and Ray, Alex and
                  Schulman, John and Hilton, Jacob and Kelton, Fraser and
                  Miller, Luke and Simens, Maddie and Askell, Amanda and
                  Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                  Ryan},
  journaltitle = {arXiv [cs.CL]},
  date         = {2022-03-04},
  eprinttype   = {arXiv},
  eprintclass  = {cs.CL},
  abstract     = {Making language models bigger does not inherently make them
                  better at following a user's intent. For example, large
                  language models can generate outputs that are untruthful,
                  toxic, or simply not helpful to the user. In other words,
                  these models are not aligned with their users. In this paper,
                  we show an avenue for aligning language models with user
                  intent on a wide range of tasks by fine-tuning with human
                  feedback. Starting with a set of labeler-written prompts and
                  prompts submitted through the OpenAI API, we collect a dataset
                  of labeler demonstrations of the desired model behavior, which
                  we use to fine-tune GPT-3 using supervised learning. We then
                  collect a dataset of rankings of model outputs, which we use
                  to further fine-tune this supervised model using reinforcement
                  learning from human feedback. We call the resulting models
                  InstructGPT. In human evaluations on our prompt distribution,
                  outputs from the 1.3B parameter InstructGPT model are
                  preferred to outputs from the 175B GPT-3, despite having 100x
                  fewer parameters. Moreover, InstructGPT models show
                  improvements in truthfulness and reductions in toxic output
                  generation while having minimal performance regressions on
                  public NLP datasets. Even though InstructGPT still makes
                  simple mistakes, our results show that fine-tuning with human
                  feedback is a promising direction for aligning language models
                  with human intent.},
  urldate      = {2024-10-31}
}

@ARTICLE{Rafailov-Direct-PreferenceModel-2023,
  title        = {Direct Preference Optimization: Your language model is
                  secretly a reward model},
  author       = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and
                  Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-05-29},
  eprinttype   = {arXiv},
  eprintclass  = {cs.LG},
  abstract     = {While large-scale unsupervised language models (LMs) learn
                  broad world knowledge and some reasoning skills, achieving
                  precise control of their behavior is difficult due to the
                  completely unsupervised nature of their training. Existing
                  methods for gaining such steerability collect human labels of
                  the relative quality of model generations and fine-tune the
                  unsupervised LM to align with these preferences, often with
                  reinforcement learning from human feedback (RLHF). However,
                  RLHF is a complex and often unstable procedure, first fitting
                  a reward model that reflects the human preferences, and then
                  fine-tuning the large unsupervised LM using reinforcement
                  learning to maximize this estimated reward without drifting
                  too far from the original model. In this paper we introduce a
                  new parameterization of the reward model in RLHF that enables
                  extraction of the corresponding optimal policy in closed form,
                  allowing us to solve the standard RLHF problem with only a
                  simple classification loss. The resulting algorithm, which we
                  call Direct Preference Optimization (DPO), is stable,
                  performant, and computationally lightweight, eliminating the
                  need for sampling from the LM during fine-tuning or performing
                  significant hyperparameter tuning. Our experiments show that
                  DPO can fine-tune LMs to align with human preferences as well
                  as or better than existing methods. Notably, fine-tuning with
                  DPO exceeds PPO-based RLHF in ability to control sentiment of
                  generations, and matches or improves response quality in
                  summarization and single-turn dialogue while being
                  substantially simpler to implement and train.},
  urldate      = {2024-10-31}
}
