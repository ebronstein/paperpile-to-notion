@ARTICLE{Kuzmin2022-ld,
  title        = {{FP8} quantization: The power of the exponent},
  author       = {Kuzmin, Andrey and Van Baalen, Mart and Ren, Yuwei and Nagel,
                  Markus and Peters, Jorn and Blankevoort, Tijmen},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-08-19},
  eprintclass  = {cs.LG},
  abstract     = {When quantizing neural networks for efficient inference,
                  low-bit integers are the go-to format for efficiency. However,
                  low-bit floating point numbers have an extra degree of
                  freedom, assigning some bits to work on an exponential scale
                  instead. This paper in-depth investigates this benefit of the
                  floating point format for neural network inference. We detail
                  the choices that can be made for the FP8 format, including the
                  important choice of the number of bits for the mantissa and
                  exponent, and show analytically in which settings these
                  choices give better performance. Then we show how these
                  findings translate to real networks, provide an efficient
                  implementation for FP8 simulation, and a new algorithm that
                  enables the learning of both the scale parameters and the
                  number of exponent bits in the FP8 format. Our chief
                  conclusion is that when doing post-training quantization for a
                  wide range of networks, the FP8 format is better than INT8 in
                  terms of accuracy, and the choice of the number of exponent
                  bits is driven by the severity of outliers in the network. We
                  also conduct experiments with quantization-aware training
                  where the difference in formats disappears as the network is
                  trained to reduce the effect of outliers.},
  urldate      = {2024-10-05}
}

@ARTICLE{Lin2023-th,
  title        = {{AWQ}: Activation-aware Weight Quantization for {LLM}
                  compression and acceleration},
  author       = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang
                  and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and
                  Dang, Xingyu and Gan, Chuang and Han, Song},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-06-01},
  eprintclass  = {cs.CL},
  abstract     = {Large language models (LLMs) have transformed numerous AI
                  applications. On-device LLM is becoming increasingly
                  important: running LLMs locally on edge devices can reduce the
                  cloud computing cost and protect users' privacy. However, the
                  astronomical model size and the limited hardware resource pose
                  significant deployment challenges. We propose Activation-aware
                  Weight Quantization (AWQ), a hardware-friendly approach for
                  LLM low-bit weight-only quantization. AWQ finds that not all
                  weights in an LLM are equally important. Protecting only 1\%
                  salient weights can greatly reduce quantization error. To
                  identify salient weight channels, we should refer to the
                  activation distribution, not weights. To avoid the
                  hardware-inefficient mix-precision quantization, we
                  mathematically derive that scaling up the salient channels can
                  reduce the quantization error. AWQ employs an equivalent
                  transformation to scale the salient weight channels to protect
                  them. The scale is determined by collecting the activation
                  statistics offline. AWQ does not rely on any backpropagation
                  or reconstruction, so it generalizes to different domains and
                  modalities without overfitting the calibration set. AWQ
                  outperforms existing work on various language modeling and
                  domain-specific benchmarks (coding and math). Thanks to better
                  generalization, it achieves excellent quantization performance
                  for instruction-tuned LMs and, for the first time, multi-modal
                  LMs. Alongside AWQ, we implement TinyChat, an efficient and
                  flexible inference framework tailored for 4-bit on-device
                  LLM/VLMs. With kernel fusion and platform-aware weight
                  packing, TinyChat offers more than 3x speedup over the
                  Huggingface FP16 implementation on both desktop and mobile
                  GPUs. It also democratizes the deployment of the 70B Llama-2
                  model on mobile GPUs.},
  urldate      = {2024-10-05}
}

@ARTICLE{Kwon2023-eq,
  title        = {Efficient memory management for large language model serving
                  with {PagedAttention}},
  author       = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng,
                  Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph
                  E and Zhang, Hao and Stoica, Ion},
  journaltitle = {arXiv [cs.LG]},
  date         = {2023-09-12},
  eprintclass  = {cs.LG},
  abstract     = {High throughput serving of large language models (LLMs)
                  requires batching sufficiently many requests at a time.
                  However, existing systems struggle because the key-value cache
                  (KV cache) memory for each request is huge and grows and
                  shrinks dynamically. When managed inefficiently, this memory
                  can be significantly wasted by fragmentation and redundant
                  duplication, limiting the batch size. To address this problem,
                  we propose PagedAttention, an attention algorithm inspired by
                  the classical virtual memory and paging techniques in
                  operating systems. On top of it, we build vLLM, an LLM serving
                  system that achieves (1) near-zero waste in KV cache memory
                  and (2) flexible sharing of KV cache within and across
                  requests to further reduce memory usage. Our evaluations show
                  that vLLM improves the throughput of popular LLMs by
                  2-4$\times$ with the same level of latency compared to the
                  state-of-the-art systems, such as FasterTransformer and Orca.
                  The improvement is more pronounced with longer sequences,
                  larger models, and more complex decoding algorithms. vLLM's
                  source code is publicly available at
                  https://github.com/vllm-project/vllm},
  urldate      = {2024-09-08}
}

@ARTICLE{Zheng2023-gy,
  title        = {{SGLang}: Efficient execution of structured language model
                  programs},
  author       = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun,
                  Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and
                  Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and
                  Barrett, Clark and Sheng, Ying},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-12-12},
  eprintclass  = {cs.AI},
  abstract     = {Large language models (LLMs) are increasingly used for complex
                  tasks that require multiple generation calls, advanced
                  prompting techniques, control flow, and structured
                  inputs/outputs. However, efficient systems are lacking for
                  programming and executing these applications. We introduce
                  SGLang, a system for efficient execution of complex language
                  model programs. SGLang consists of a frontend language and a
                  runtime. The frontend simplifies programming with primitives
                  for generation and parallelism control. The runtime
                  accelerates execution with novel optimizations like
                  RadixAttention for KV cache reuse and compressed finite state
                  machines for faster structured output decoding. Experiments
                  show that SGLang achieves up to 6.4x higher throughput
                  compared to state-of-the-art inference systems on various
                  large language and multi-modal models on tasks including agent
                  control, logical reasoning, few-shot learning benchmarks, JSON
                  decoding, retrieval-augmented generation pipelines, and
                  multi-turn chat. The code is publicly available at
                  https://github.com/sgl-project/sglang},
  urldate      = {2024-09-08}
}

@ARTICLE{Dubey2024-dl,
  title        = {The Llama 3 herd of models},
  author       = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and
                  Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and
                  Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela
                  and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and
                  Mitra, Archi and Sravankumar, Archie and Korenev, Artem and
                  Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez,
                  Aurelien and Gregerson, Austen and Spataru, Ava and Roziere,
                  Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie
                  and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and
                  Marra, Chris and McConnell, Chris and Keller, Christian and
                  Touret, Christophe and Wu, Chunyang and Wong, Corinne and
                  Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius,
                  Damien and Song, Daniel and Pintz, Danielle and Livshits,
                  Danny and Esiobu, David and Choudhary, Dhruv and Mahajan,
                  Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes,
                  Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova,
                  Elina and Dinan, Emily and Smith, Eric Michael and Radenovic,
                  Filip and Zhang, Frank and Synnaeve, Gabriel and Lee,
                  Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and
                  Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and
                  Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron,
                  Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and
                  Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet,
                  Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and
                  Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der
                  Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee,
                  Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and
                  Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna
                  and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and
                  Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala,
                  Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and
                  Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini,
                  Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley
                  and Bhalla, Kunal and Rantala-Yeary, Lauren and van der
                  Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins,
                  Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and
                  Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and
                  Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and
                  Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and
                  Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and
                  Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan,
                  Mona and Goyal, Naman and Torabi, Narjes and Bashlykov,
                  Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and
                  Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and
                  Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng,
                  Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan,
                  Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and
                  Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and
                  Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic,
                  Robert and Raileanu, Roberta and Girdhar, Rohit and Patel,
                  Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly,
                  Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang,
                  Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh,
                  Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov,
                  Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy,
                  Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti
                  and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and
                  Whitman, Spencer and Sootla, Sten and Collot, Stephane and
                  Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar
                  and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and
                  Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor
                  and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and
                  Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and
                  Gonguet, Vincent and Do, Virginie and Vogeti, Vish and
                  Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu,
                  Wenyin and Meers, Whitney and Martinet, Xavier and Wang,
                  Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia,
                  Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur,
                  Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and
                  Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert,
                  Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and
                  Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and
                  Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi,
                  Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon,
                  Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex
                  and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda
                  and Sangani, Amit and Yunus, Anam and Lupu, Andrei and
                  Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho,
                  Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani,
                  Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury,
                  Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and
                  Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer,
                  Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth
                  and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and
                  Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and
                  Spence, Brandon and Stojkovic, Brani and Gamido, Brian and
                  Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia,
                  Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao
                  and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and
                  Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon
                  and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt,
                  Danny and Adkins, David and Xu, David and Testuggine, Davide
                  and David, Delia and Parikh, Devi and Liskovich, Diana and
                  Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin
                  and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine
                  and Presani, Eleonora and Hahn, Emily and Wood, Emily and
                  Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and
                  Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng
                  and Ozgenel, Firat and Caggioni, Francesco and Guzmán,
                  Francisco and Kanayet, Frank and Seide, Frank and Florez,
                  Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and
                  Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman,
                  Grant and Sizov, Grigory and {Guangyi} and {Zhang} and
                  Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and
                  Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph,
                  Harrison and Suk, Helen and Aspegren, Henry and Goldman,
                  Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor
                  and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and
                  Geboski, James and Kohli, James and Asher, Japhet and Gaya,
                  Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan,
                  Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul,
                  Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and
                  Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie,
                  Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang,
                  Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and
                  Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun
                  and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena,
                  Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and
                  Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg,
                  Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and
                  Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich,
                  Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani,
                  Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus,
                  Martynas and Hasson, Matan and Lennie, Matthew and Reso,
                  Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya
                  and Keneally, Meghan and Seltzer, Michael L and Valko, Michal
                  and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and
                  Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang,
                  Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari,
                  Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks,
                  Natascha and White, Natasha and Bawa, Navyata and Singhal,
                  Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay
                  Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and
                  Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and
                  Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab,
                  Paul and Balaji, Pavan and Rittner, Pedro and Bontrager,
                  Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina
                  and Ratanchandani, Prashant and Yuvraj, Pritish and Liang,
                  Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and
                  Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li,
                  Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky
                  and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and
                  Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt,
                  Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru
                  and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh
                  and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin,
                  Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang,
                  Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal,
                  Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max,
                  Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield,
                  Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho,
                  Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury,
                  Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and
                  Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li,
                  Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy
                  and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria
                  and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay
                  Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad
                  and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov,
                  Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and
                  Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang,
                  Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and
                  Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia,
                  Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying
                  and Adi, Yossi and Nam, Youngjin and {Yu} and {Wang} and Hao,
                  Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito,
                  Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu
                  and Zhao, Zhiwei},
  journaltitle = {arXiv [cs.AI]},
  date         = {2024-07-31},
  eprintclass  = {cs.AI},
  abstract     = {Modern artificial intelligence (AI) systems are powered by
                  foundation models. This paper presents a new set of foundation
                  models, called Llama 3. It is a herd of language models that
                  natively support multilinguality, coding, reasoning, and tool
                  usage. Our largest model is a dense Transformer with 405B
                  parameters and a context window of up to 128K tokens. This
                  paper presents an extensive empirical evaluation of Llama 3.
                  We find that Llama 3 delivers comparable quality to leading
                  language models such as GPT-4 on a plethora of tasks. We
                  publicly release Llama 3, including pre-trained and
                  post-trained versions of the 405B parameter language model and
                  our Llama Guard 3 model for input and output safety. The paper
                  also presents the results of experiments in which we integrate
                  image, video, and speech capabilities into Llama 3 via a
                  compositional approach. We observe this approach performs
                  competitively with the state-of-the-art on image, video, and
                  speech recognition tasks. The resulting models are not yet
                  being broadly released as they are still under development.},
  urldate      = {2024-09-03}
}

@ARTICLE{Yu2022-wh,
  title        = {Orca: A distributed serving system for Transformer-based
                  generative models},
  author       = {Yu, Gyeong-In and Jeong, Joo Seong},
  journaltitle = {Oper Syst Des Implement},
  pages        = {521--538},
  date         = {2022},
  abstract     = {Large-scale Transformer-based models trained for generation
                  tasks (e.g., GPT-3) have recently attracted huge interest,
                  emphasizing the need for system support for serving models in
                  this family. Since these models generate a next token in an
                  autoregressive manner, one has to run the model multiple times
                  to process an inference request where each iteration of the
                  model generates a single output token for the request.
                  However, existing systems for inference serving do not perform
                  well on this type of workload that has a multi-iteration
                  characteristic, due to their inflexible scheduling mechanism
                  that cannot change the current batch of requests being
                  processed; requests that have finished earlier than other
                  requests in a batch cannot return to the client, while newly
                  arrived requests have to wait until the current batch
                  completely finishes. In this paper, we propose iteration-level
                  scheduling, a new scheduling mechanism that schedules
                  execution at the granularity of iteration (instead of request)
                  where the scheduler invokes the execution engine to run only a
                  single iteration of the model on the batch. In addition, to
                  apply batching and iteration-level scheduling to a Transformer
                  model at the same time, we suggest selective batching, which
                  applies batching only to a selected set of operations. Based
                  on these two techniques, we have implemented a distributed
                  serving system called ORCA, with additional designs for
                  scalability to models with hundreds of billions of parameters.
                  Our evaluation on a GPT-3 175B model shows that ORCA can
                  significantly outperform NVIDIA FasterTransformer in terms of
                  both latency and throughput: 36.9× throughput improvement at
                  the same level of latency.},
  urldate      = {2024-09-09}
}

@ARTICLE{Agrawal2024-zx,
  title        = {Taming throughput-latency tradeoff in {LLM} inference with
                  sarathi-serve},
  author       = {Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan,
                  Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and
                  Tumanov, Alexey and Ramjee, Ramachandran},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-03-04},
  eprintclass  = {cs.LG},
  abstract     = {Each LLM serving request goes through two phases. The first is
                  prefill which processes the entire input prompt and produces
                  the first output token and the second is decode which
                  generates the rest of output tokens, one-at-a-time. Prefill
                  iterations have high latency but saturate GPU compute due to
                  parallel processing of the input prompt. In contrast, decode
                  iterations have low latency but also low compute utilization
                  because a decode iteration processes only a single token per
                  request. This makes batching highly effective for decodes and
                  consequently for overall throughput. However, batching
                  multiple requests leads to an interleaving of prefill and
                  decode iterations which makes it challenging to achieve both
                  high throughput and low latency. We introduce an efficient LLM
                  inference scheduler, Sarathi-Serve, to address this
                  throughput-latency tradeoff. Sarathi-Serve introduces
                  chunked-prefills which splits a prefill request into near
                  equal sized chunks and creates stall-free schedules that adds
                  new requests in a batch without pausing ongoing decodes.
                  Stall-free scheduling unlocks the opportunity to improve
                  throughput with large batch sizes while minimizing the effect
                  of batching on latency. Furthermore, uniform batches in
                  Sarathi-Serve ameliorate the imbalance between iterations
                  resulting in minimal pipeline bubbles. Our techniques yield
                  significant improvements in inference performance across
                  models and hardware under tail latency constraints. For
                  Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving
                  capacity and up to 3.7x higher serving capacity for the Yi-34B
                  model on two A100 GPUs as compared to vLLM. When used with
                  pipeline parallelism on Falcon-180B, Sarathi-Serve provides up
                  to 5.6x gain in the end-to-end serving capacity. The source
                  code for Sarathi-Serve is available at
                  https://github.com/microsoft/sarathi-serve.},
  urldate      = {2024-09-09}
}

@ARTICLE{Khattab2023-cb,
  title        = {{DSPy}: Compiling declarative language model calls into
                  self-improving pipelines},
  author       = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and
                  Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and
                  Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T and
                  Moazam, Hanna and Miller, Heather and Zaharia, Matei and
                  Potts, Christopher},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-05},
  eprintclass  = {cs.CL},
  abstract     = {The ML community is rapidly exploring techniques for prompting
                  language models (LMs) and for stacking them into pipelines
                  that solve complex tasks. Unfortunately, existing LM pipelines
                  are typically implemented using hard-coded "prompt templates",
                  i.e. lengthy strings discovered via trial and error. Toward a
                  more systematic approach for developing and optimizing LM
                  pipelines, we introduce DSPy, a programming model that
                  abstracts LM pipelines as text transformation graphs, i.e.
                  imperative computational graphs where LMs are invoked through
                  declarative modules. DSPy modules are parameterized, meaning
                  they can learn (by creating and collecting demonstrations) how
                  to apply compositions of prompting, finetuning, augmentation,
                  and reasoning techniques. We design a compiler that will
                  optimize any DSPy pipeline to maximize a given metric. We
                  conduct two case studies, showing that succinct DSPy programs
                  can express and optimize sophisticated LM pipelines that
                  reason about math word problems, tackle multi-hop retrieval,
                  answer complex questions, and control agent loops. Within
                  minutes of compiling, a few lines of DSPy allow GPT-3.5 and
                  llama2-13b-chat to self-bootstrap pipelines that outperform
                  standard few-shot prompting (generally by over 25\% and 65\%,
                  respectively) and pipelines with expert-created demonstrations
                  (by up to 5-46\% and 16-40\%, respectively). On top of that,
                  DSPy programs compiled to open and relatively small LMs like
                  770M-parameter T5 and llama2-13b-chat are competitive with
                  approaches that rely on expert-written prompt chains for
                  proprietary GPT-3.5. DSPy is available at
                  https://github.com/stanfordnlp/dspy},
  urldate      = {2024-09-23}
}

@ARTICLE{Packer2023-fp,
  title        = {{MemGPT}: Towards {LLMs} as operating systems},
  author       = {Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang,
                  Vivian and Patil, Shishir G and Stoica, Ion and Gonzalez,
                  Joseph E},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-10-12},
  eprintclass  = {cs.AI},
  abstract     = {Large language models (LLMs) have revolutionized AI, but are
                  constrained by limited context windows, hindering their
                  utility in tasks like extended conversations and document
                  analysis. To enable using context beyond limited context
                  windows, we propose virtual context management, a technique
                  drawing inspiration from hierarchical memory systems in
                  traditional operating systems that provide the appearance of
                  large memory resources through data movement between fast and
                  slow memory. Using this technique, we introduce MemGPT
                  (Memory-GPT), a system that intelligently manages different
                  memory tiers in order to effectively provide extended context
                  within the LLM's limited context window, and utilizes
                  interrupts to manage control flow between itself and the user.
                  We evaluate our OS-inspired design in two domains where the
                  limited context windows of modern LLMs severely handicaps
                  their performance: document analysis, where MemGPT is able to
                  analyze large documents that far exceed the underlying LLM's
                  context window, and multi-session chat, where MemGPT can
                  create conversational agents that remember, reflect, and
                  evolve dynamically through long-term interactions with their
                  users. We release MemGPT code and data for our experiments at
                  https://memgpt.ai.},
  urldate      = {2024-09-23}
}

@ARTICLE{Yang2022-cq,
  title        = {Tensor programs {V}: Tuning large neural networks via
                  zero-shot hyperparameter transfer},
  author       = {Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor,
                  Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and
                  Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-03-07},
  eprintclass  = {cs.LG},
  abstract     = {Hyperparameter (HP) tuning in deep learning is an expensive
                  process, prohibitively so for neural networks (NNs) with
                  billions of parameters. We show that, in the recently
                  discovered Maximal Update Parametrization (muP), many optimal
                  HPs remain stable even as model size changes. This leads to a
                  new HP tuning paradigm we call muTransfer: parametrize the
                  target model in muP, tune the HP indirectly on a smaller
                  model, and zero-shot transfer them to the full-sized model,
                  i.e., without directly tuning the latter at all. We verify
                  muTransfer on Transformer and ResNet. For example, 1) by
                  transferring pretraining HPs from a model of 13M parameters,
                  we outperform published numbers of BERT-large (350M
                  parameters), with a total tuning cost equivalent to
                  pretraining BERT-large once; 2) by transferring from 40M
                  parameters, we outperform published numbers of the 6.7B GPT-3
                  model, with tuning cost only 7\% of total pretraining cost. A
                  Pytorch implementation of our technique can be found at
                  github.com/microsoft/mup and installable via `pip install
                  mup`.},
  urldate      = {2024-10-14}
}

@ARTICLE{Kaplan2020-cc,
  title        = {Scaling laws for neural language models},
  author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown,
                  Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and
                  Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journaltitle = {arXiv [cs.LG]},
  date         = {2020-01-22},
  eprintclass  = {cs.LG},
  abstract     = {We study empirical scaling laws for language model performance
                  on the cross-entropy loss. The loss scales as a power-law with
                  model size, dataset size, and the amount of compute used for
                  training, with some trends spanning more than seven orders of
                  magnitude. Other architectural details such as network width
                  or depth have minimal effects within a wide range. Simple
                  equations govern the dependence of overfitting on
                  model/dataset size and the dependence of training speed on
                  model size. These relationships allow us to determine the
                  optimal allocation of a fixed compute budget. Larger models
                  are significantly more sample-efficient, such that optimally
                  compute-efficient training involves training very large models
                  on a relatively modest amount of data and stopping
                  significantly before convergence.},
  urldate      = {2024-10-14}
}

@ARTICLE{Hendrycks2020-of,
  title        = {Measuring massive multitask language understanding},
  author       = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou,
                  Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journaltitle = {arXiv [cs.CY]},
  date         = {2020-09-07},
  eprintclass  = {cs.CY},
  abstract     = {We propose a new test to measure a text model's multitask
                  accuracy. The test covers 57 tasks including elementary
                  mathematics, US history, computer science, law, and more. To
                  attain high accuracy on this test, models must possess
                  extensive world knowledge and problem solving ability. We find
                  that while most recent models have near random-chance
                  accuracy, the very largest GPT-3 model improves over random
                  chance by almost 20 percentage points on average. However, on
                  every one of the 57 tasks, the best models still need
                  substantial improvements before they can reach expert-level
                  accuracy. Models also have lopsided performance and frequently
                  do not know when they are wrong. Worse, they still have
                  near-random accuracy on some socially important subjects such
                  as morality and law. By comprehensively evaluating the breadth
                  and depth of a model's academic and professional
                  understanding, our test can be used to analyze models across
                  many tasks and to identify important shortcomings.},
  urldate      = {2024-09-16}
}

@ARTICLE{Oren2023-cf,
  title        = {Proving test set contamination in black box language models},
  author       = {Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and
                  Ladhak, Faisal and Hashimoto, Tatsunori B},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-26},
  eprintclass  = {cs.CL},
  abstract     = {Large language models are trained on vast amounts of internet
                  data, prompting concerns and speculation that they have
                  memorized public benchmarks. Going from speculation to proof
                  of contamination is challenging, as the pretraining data used
                  by proprietary models are often not publicly accessible. We
                  show that it is possible to provide provable guarantees of
                  test set contamination in language models without access to
                  pretraining data or model weights. Our approach leverages the
                  fact that when there is no data contamination, all orderings
                  of an exchangeable benchmark should be equally likely. In
                  contrast, the tendency for language models to memorize example
                  order means that a contaminated language model will find
                  certain canonical orderings to be much more likely than
                  others. Our test flags potential contamination whenever the
                  likelihood of a canonically ordered benchmark dataset is
                  significantly higher than the likelihood after shuffling the
                  examples. We demonstrate that our procedure is sensitive
                  enough to reliably prove test set contamination in challenging
                  situations, including models as small as 1.4 billion
                  parameters, on small test sets of only 1000 examples, and
                  datasets that appear only a few times in the pretraining
                  corpus. Using our test, we audit five popular publicly
                  accessible language models for test set contamination and find
                  little evidence for pervasive contamination.},
  urldate      = {2024-09-16}
}

@ARTICLE{Oehrn2024-ct,
  title        = {Chronic adaptive deep brain stimulation versus conventional
                  stimulation in Parkinson's disease: a blinded randomized
                  feasibility trial},
  author       = {Oehrn, Carina R and Cernera, Stephanie and Hammer, Lauren H
                  and Shcherbakova, Maria and Yao, Jiaang and Hahn, Amelia and
                  Wang, Sarah and Ostrem, Jill L and Little, Simon and Starr,
                  Philip A},
  journaltitle = {Nat. Med.},
  publisher    = {Springer Science and Business Media LLC},
  pages        = {1--12},
  date         = {2024-08-19},
  abstract     = {Deep brain stimulation (DBS) is a widely used therapy for
                  Parkinson's disease (PD) but lacks dynamic responsiveness to
                  changing clinical and neural states. Feedback control might
                  improve therapeutic effectiveness, but the optimal control
                  strategy and additional benefits of 'adaptive'
                  neurostimulation are unclear. Here we present the results of a
                  blinded randomized cross-over pilot trial aimed at determining
                  the neural correlates of specific motor signs in individuals
                  with PD and the feasibility of using these signals to drive
                  adaptive DBS. Four male patients with PD were recruited from a
                  population undergoing DBS implantation for motor fluctuations,
                  with each patient receiving adaptive DBS and continuous DBS.
                  We identified stimulation-entrained gamma oscillations in the
                  subthalamic nucleus or motor cortex as optimal markers of high
                  versus low dopaminergic states and their associated residual
                  motor signs in all four patients. We then demonstrated
                  improved motor symptoms and quality of life with adaptive
                  compared to clinically optimized standard stimulation. The
                  results of this pilot trial highlight the promise of
                  personalized adaptive neurostimulation in PD based on
                  data-driven selection of neural signals. Furthermore, these
                  findings provide the foundation for further larger clinical
                  trials to evaluate the efficacy of personalized adaptive
                  neurostimulation in PD and other neurological disorders.
                  ClinicalTrials.gov registration: NCT03582891 .},
  urldate      = {2024-09-20},
  language     = {en}
}

@ARTICLE{Malkov2020-km,
  title        = {Efficient and robust approximate nearest neighbor search using
                  hierarchical Navigable Small World graphs},
  author       = {Malkov, Yu A and Yashunin, D A},
  journaltitle = {IEEE Trans. Pattern Anal. Mach. Intell.},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume       = {42},
  issue        = {4},
  pages        = {824--836},
  date         = {2020-04},
  abstract     = {We present a new approach for the approximate K-nearest
                  neighbor search based on navigable small world graphs with
                  controllable hierarchy (Hierarchical NSW, HNSW). The proposed
                  solution is fully graph-based, without any need for additional
                  search structures (typically used at the coarse search stage
                  of the most proximity graph techniques). Hierarchical NSW
                  incrementally builds a multi-layer structure consisting of a
                  hierarchical set of proximity graphs (layers) for nested
                  subsets of the stored elements. The maximum layer in which an
                  element is present is selected randomly with an exponentially
                  decaying probability distribution. This allows producing
                  graphs similar to the previously studied Navigable Small World
                  (NSW) structures while additionally having the links separated
                  by their characteristic distance scales. Starting the search
                  from the upper layer together with utilizing the scale
                  separation boosts the performance compared to NSW and allows a
                  logarithmic complexity scaling. Additional employment of a
                  heuristic for selecting proximity graph neighbors
                  significantly increases performance at high recall and in case
                  of highly clustered data. Performance evaluation has
                  demonstrated that the proposed general metric space search
                  index is able to strongly outperform previous opensource
                  state-of-the-art vector-only approaches. Similarity of the
                  algorithm to the skip list structure allows straightforward
                  balanced distributed implementation.},
  urldate      = {2024-09-21},
  language     = {en}
}

@ARTICLE{Santhanam2021-vo,
  title        = {{ColBERTv2}: Effective and efficient retrieval via lightweight
                  late interaction},
  author       = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and
                  Potts, Christopher and Zaharia, Matei},
  journaltitle = {arXiv [cs.IR]},
  date         = {2021-12-02},
  eprintclass  = {cs.IR},
  abstract     = {Neural information retrieval (IR) has greatly advanced search
                  and other knowledge-intensive language tasks. While many
                  neural IR methods encode queries and documents into
                  single-vector representations, late interaction models produce
                  multi-vector representations at the granularity of each token
                  and decompose relevance modeling into scalable token-level
                  computations. This decomposition has been shown to make late
                  interaction more effective, but it inflates the space
                  footprint of these models by an order of magnitude. In this
                  work, we introduce ColBERTv2, a retriever that couples an
                  aggressive residual compression mechanism with a denoised
                  supervision strategy to simultaneously improve the quality and
                  space footprint of late interaction. We evaluate ColBERTv2
                  across a wide range of benchmarks, establishing
                  state-of-the-art quality within and outside the training
                  domain while reducing the space footprint of late interaction
                  models by 6--10$\times$.},
  urldate      = {2024-09-21}
}

@ARTICLE{Chen2023-vp,
  title        = {Accelerating large language model decoding with speculative
                  sampling},
  author       = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and
                  Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-02-02},
  eprintclass  = {cs.CL},
  abstract     = {We present speculative sampling, an algorithm for accelerating
                  transformer decoding by enabling the generation of multiple
                  tokens from each transformer call. Our algorithm relies on the
                  observation that the latency of parallel scoring of short
                  continuations, generated by a faster but less powerful draft
                  model, is comparable to that of sampling a single token from
                  the larger target model. This is combined with a novel
                  modified rejection sampling scheme which preserves the
                  distribution of the target model within hardware numerics. We
                  benchmark speculative sampling with Chinchilla, a 70 billion
                  parameter language model, achieving a 2-2.5x decoding speedup
                  in a distributed setup, without compromising the sample
                  quality or making modifications to the model itself.},
  urldate      = {2024-09-14}
}

@ARTICLE{Cai2024-zc,
  title        = {Medusa: Simple {LLM} inference acceleration framework with
                  multiple decoding heads},
  author       = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng,
                  Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-01-19},
  eprintclass  = {cs.LG},
  abstract     = {Large Language Models (LLMs) employ auto-regressive decoding
                  that requires sequential computation, with each step reliant
                  on the previous one's output. This creates a bottleneck as
                  each step necessitates moving the full model parameters from
                  High-Bandwidth Memory (HBM) to the accelerator's cache. While
                  methods such as speculative decoding have been suggested to
                  address this issue, their implementation is impeded by the
                  challenges associated with acquiring and maintaining a
                  separate draft model. In this paper, we present Medusa, an
                  efficient method that augments LLM inference by adding extra
                  decoding heads to predict multiple subsequent tokens in
                  parallel. Using a tree-based attention mechanism, Medusa
                  constructs multiple candidate continuations and verifies them
                  simultaneously in each decoding step. By leveraging parallel
                  processing, Medusa substantially reduces the number of
                  decoding steps required. We present two levels of fine-tuning
                  procedures for Medusa to meet the needs of different use
                  cases: Medusa-1: Medusa is directly fine-tuned on top of a
                  frozen backbone LLM, enabling lossless inference acceleration.
                  Medusa-2: Medusa is fine-tuned together with the backbone LLM,
                  enabling better prediction accuracy of Medusa heads and higher
                  speedup but needing a special training recipe that preserves
                  the backbone model's capabilities. Moreover, we propose
                  several extensions that improve or expand the utility of
                  Medusa, including a self-distillation to handle situations
                  where no training data is available and a typical acceptance
                  scheme to boost the acceptance rate while maintaining
                  generation quality. We evaluate Medusa on models of various
                  sizes and training procedures. Our experiments demonstrate
                  that Medusa-1 can achieve over 2.2x speedup without
                  compromising generation quality, while Medusa-2 further
                  improves the speedup to 2.3-3.6x.},
  urldate      = {2024-09-14}
}

@INPROCEEDINGS{Schulman2013-pa,
  title     = {Finding locally optimal, collision-free trajectories with
               sequential convex optimization},
  author    = {Schulman, John and Ho, Jonathan and Lee, Alex and Awwal, Ibrahim
               and Bradlow, Henry and Abbeel, Pieter},
  publisher = {Robotics: Science and Systems Foundation},
  date      = {2013-06-23},
  abstract  = {We present a novel approach for incorporating collision avoidance
               into trajectory optimization as a method of solving robotic
               motion planning problems. At the core of our approach are (i) A
               sequential convex optimization procedure, which penalizes
               collisions with a hinge loss and increases the penalty
               coefficients in an outer loop as necessary. (ii) An efficient
               formulation of the no-collisions constraint that directly
               considers continuous-time safety and enables the algorithm to
               reliably solve motion planning problems, including problems
               involving thin and complex obstacles. We benchmarked our
               algorithm against several other motion planning algorithms,
               solving a suite of 7-degree-of-freedom (DOF) arm-planning
               problems and 18-DOF full-body planning problems. We compared
               against sampling-based planners from OMPL, and we also compared
               to CHOMP, a leading approach for trajectory optimization. Our
               algorithm was faster than the alternatives, solved more problems,
               and yielded higher quality paths. Experimental evaluation on the
               following additional problem types also confirmed the speed and
               effectiveness of our approach: (i) Planning foot placements with
               34 degrees of freedom (28 joints + 6 DOF pose) of the Atlas
               humanoid robot as it maintains static stability and has to
               negotiate environmental constraints. (ii) Industrial box picking.
               (iii) Real-world motion planning for the PR2 that requires
               considering all degrees of freedom at the same time.}
}

@ARTICLE{Yao2022-sk,
  title        = {{WebShop}: Towards scalable real-world web interaction with
                  grounded language agents},
  author       = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan,
                  Karthik},
  journaltitle = {arXiv [cs.CL]},
  date         = {2022-07-04},
  eprintclass  = {cs.CL},
  abstract     = {Existing benchmarks for grounding language in interactive
                  environments either lack real-world linguistic elements, or
                  prove difficult to scale up due to substantial human
                  involvement in the collection of data or feedback signals. To
                  bridge this gap, we develop WebShop -- a simulated e-commerce
                  website environment with $1.18$ million real-world products
                  and $12,087$ crowd-sourced text instructions. Given a text
                  instruction specifying a product requirement, an agent needs
                  to navigate multiple types of webpages and issue diverse
                  actions to find, customize, and purchase an item. WebShop
                  provides several challenges for language grounding including
                  understanding compositional instructions, query
                  (re-)formulation, comprehending and acting on noisy text in
                  webpages, and performing strategic exploration. We collect
                  over $1,600$ human demonstrations for the task, and train and
                  evaluate a diverse range of agents using reinforcement
                  learning, imitation learning, and pre-trained image and
                  language models. Our best model achieves a task success rate
                  of $29\%$, which outperforms rule-based heuristics ($9.6\%$)
                  but is far lower than human expert performance ($59\%$). We
                  also analyze agent and human trajectories and ablate various
                  model components to provide insights for developing future
                  agents with stronger language understanding and decision
                  making abilities. Finally, we show that agents trained on
                  WebShop exhibit non-trivial sim-to-real transfer when
                  evaluated on amazon.com and ebay.com, indicating the potential
                  value of WebShop in developing practical web-based agents that
                  can operate in the wild.},
  urldate      = {2024-09-15}
}

@INPROCEEDINGS{Kraska2018-ob,
  title      = {The Case for Learned Index Structures},
  author     = {Kraska, Tim and Beutel, Alex and Chi, Ed H and Dean, Jeffrey and
                Polyzotis, Neoklis},
  booktitle  = {Proceedings of the 2018 International Conference on Management
                of Data},
  publisher  = {ACM},
  location   = {New York, NY, USA},
  eventtitle = {SIGMOD/PODS '18: International Conference on Management of Data},
  venue      = {Houston TX USA},
  date       = {2018-05-27},
  urldate    = {2024-09-23},
  language   = {en}
}

@ARTICLE{Zhilin2018-se,
  title        = {{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop
                  Question Answering},
  author       = {Zhilin, Yang and Peng, Qi and Saizheng, Zhang and Yoshua,
                  Bengio and William, W Cohen and Ruslan, Salakhutdinov and
                  Christopher, D Manning},
  journaltitle = {arXiv [cs.CL]},
  date         = {2018-09-25},
  eprintclass  = {cs.CL},
  abstract     = {Existing question answering (QA) datasets fail to train QA
                  systems to perform complex reasoning and provide explanations
                  for answers. We introduce HotpotQA, a new dataset with 113k
                  Wikipedia-based question-answer pairs with four key features:
                  (1) the questions require finding and reasoning over multiple
                  supporting documents to answer; (2) the questions are diverse
                  and not constrained to any pre-existing knowledge bases or
                  knowledge schemas; (3) we provide sentence-level supporting
                  facts required for reasoning, allowing QA systems to reason
                  with strong supervision and explain the predictions; (4) we
                  offer a new type of factoid comparison questions to test QA
                  systems' ability to extract relevant facts and perform
                  necessary comparison. We show that HotpotQA is challenging for
                  the latest QA systems, and the supporting facts enable models
                  to improve performance and make explainable predictions.},
  urldate      = {2024-09-29}
}

@ARTICLE{Zhou2023-wy,
  title        = {{WebArena}: A realistic web environment for building
                  autonomous agents},
  author       = {Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and
                  Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou,
                  Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and
                  Neubig, Graham},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-07-25},
  eprintclass  = {cs.AI},
  abstract     = {With advances in generative AI, there is now potential for
                  autonomous agents to manage daily tasks via natural language
                  commands. However, current agents are primarily created and
                  tested in simplified synthetic environments, leading to a
                  disconnect with real-world scenarios. In this paper, we build
                  an environment for language-guided agents that is highly
                  realistic and reproducible. Specifically, we focus on agents
                  that perform tasks on the web, and create an environment with
                  fully functional websites from four common domains:
                  e-commerce, social forum discussions, collaborative software
                  development, and content management. Our environment is
                  enriched with tools (e.g., a map) and external knowledge bases
                  (e.g., user manuals) to encourage human-like task-solving.
                  Building upon our environment, we release a set of benchmark
                  tasks focusing on evaluating the functional correctness of
                  task completions. The tasks in our benchmark are diverse,
                  long-horizon, and designed to emulate tasks that humans
                  routinely perform on the internet. We experiment with several
                  baseline agents, integrating recent techniques such as
                  reasoning before acting. The results demonstrate that solving
                  complex tasks is challenging: our best GPT-4-based agent only
                  achieves an end-to-end task success rate of 14.41\%,
                  significantly lower than the human performance of 78.24\%.
                  These results highlight the need for further development of
                  robust agents, that current state-of-the-art large language
                  models are far from perfect performance in these real-life
                  tasks, and that WebArena can be used to measure such progress.},
  urldate      = {2024-09-29}
}

@ARTICLE{Mialon2023-pt,
  title        = {{GAIA}: a benchmark for General {AI} Assistants},
  author       = {Mialon, Grégoire and Fourrier, Clémentine and Swift, Craig and
                  Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-11-21},
  eprintclass  = {cs.CL},
  abstract     = {We introduce GAIA, a benchmark for General AI Assistants that,
                  if solved, would represent a milestone in AI research. GAIA
                  proposes real-world questions that require a set of
                  fundamental abilities such as reasoning, multi-modality
                  handling, web browsing, and generally tool-use proficiency.
                  GAIA questions are conceptually simple for humans yet
                  challenging for most advanced AIs: we show that human
                  respondents obtain 92\% vs. 15\% for GPT-4 equipped with
                  plugins. This notable performance disparity contrasts with the
                  recent trend of LLMs outperforming humans on tasks requiring
                  professional skills in e.g. law or chemistry. GAIA's
                  philosophy departs from the current trend in AI benchmarks
                  suggesting to target tasks that are ever more difficult for
                  humans. We posit that the advent of Artificial General
                  Intelligence (AGI) hinges on a system's capability to exhibit
                  similar robustness as the average human does on such
                  questions. Using GAIA's methodology, we devise 466 questions
                  and their answer. We release our questions while retaining
                  answers to 300 of them to power a leader-board available at
                  https://huggingface.co/gaia-benchmark.},
  urldate      = {2024-09-29}
}

@ARTICLE{Jimenez2023-hp,
  title        = {{SWE}-bench: Can language models resolve real-world {GitHub}
                  issues?},
  author       = {Jimenez, Carlos E and Yang, John and Wettig, Alexander and
                  Yao, Shunyu and Pei, Kexin and {Ofir Press} and Narasimhan,
                  Karthik},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-10},
  eprintclass  = {cs.CL},
  abstract     = {Language models have outpaced our ability to evaluate them
                  effectively, but for their future development it is essential
                  to study the frontier of their capabilities. We find
                  real-world software engineering to be a rich, sustainable, and
                  challenging testbed for evaluating the next generation of
                  language models. To this end, we introduce SWE-bench, an
                  evaluation framework consisting of $2,294$ software
                  engineering problems drawn from real GitHub issues and
                  corresponding pull requests across $12$ popular Python
                  repositories. Given a codebase along with a description of an
                  issue to be resolved, a language model is tasked with editing
                  the codebase to address the issue. Resolving issues in
                  SWE-bench frequently requires understanding and coordinating
                  changes across multiple functions, classes, and even files
                  simultaneously, calling for models to interact with execution
                  environments, process extremely long contexts and perform
                  complex reasoning that goes far beyond traditional code
                  generation tasks. Our evaluations show that both
                  state-of-the-art proprietary models and our fine-tuned model
                  SWE-Llama can resolve only the simplest issues. The
                  best-performing model, Claude 2, is able to solve a mere
                  $1.96$\% of the issues. Advances on SWE-bench represent steps
                  towards LMs that are more practical, intelligent, and
                  autonomous.},
  urldate      = {2024-09-29}
}

@ARTICLE{Austin2021-pm,
  title        = {Program synthesis with large language models},
  author       = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma,
                  Maarten and Michalewski, Henryk and Dohan, David and Jiang,
                  Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and
                  Sutton, Charles},
  journaltitle = {arXiv [cs.PL]},
  date         = {2021-08-15},
  eprintclass  = {cs.PL},
  abstract     = {This paper explores the limits of the current generation of
                  large language models for program synthesis in general purpose
                  programming languages. We evaluate a collection of such models
                  (with between 244M and 137B parameters) on two new benchmarks,
                  MBPP and MathQA-Python, in both the few-shot and fine-tuning
                  regimes. Our benchmarks are designed to measure the ability of
                  these models to synthesize short Python programs from natural
                  language descriptions. The Mostly Basic Programming Problems
                  (MBPP) dataset contains 974 programming tasks, designed to be
                  solvable by entry-level programmers. The MathQA-Python
                  dataset, a Python version of the MathQA benchmark, contains
                  23914 problems that evaluate the ability of the models to
                  synthesize code from more complex text. On both datasets, we
                  find that synthesis performance scales log-linearly with model
                  size. Our largest models, even without finetuning on a code
                  dataset, can synthesize solutions to 59.6 percent of the
                  problems from MBPP using few-shot learning with a
                  well-designed prompt. Fine-tuning on a held-out portion of the
                  dataset improves performance by about 10 percentage points
                  across most model sizes. On the MathQA-Python dataset, the
                  largest fine-tuned model achieves 83.8 percent accuracy. Going
                  further, we study the model's ability to engage in dialog
                  about code, incorporating human feedback to improve its
                  solutions. We find that natural language feedback from a human
                  halves the error rate compared to the model's initial
                  prediction. Additionally, we conduct an error analysis to shed
                  light on where these models fall short and what types of
                  programs are most difficult to generate. Finally, we explore
                  the semantic grounding of these models by fine-tuning them to
                  predict the results of program execution. We find that even
                  our best models are generally unable to predict the output of
                  a program given a specific input.},
  urldate      = {2024-09-29}
}

@ARTICLE{Li2024-bz,
  title        = {{WebSuite}: Systematically evaluating why web agents fail},
  author       = {Li, Eric and Waldo, Jim},
  journaltitle = {arXiv [cs.SE]},
  date         = {2024-05-31},
  eprintclass  = {cs.SE},
  abstract     = {We describe WebSuite, the first diagnostic benchmark for
                  generalist web agents, designed to systematically evaluate why
                  agents fail. Advances in AI have led to the rise of numerous
                  web agents that autonomously operate a browser to complete
                  tasks. However, most existing benchmarks focus on strictly
                  measuring whether an agent can or cannot complete a task,
                  without giving insight on why. In this paper, we 1) develop a
                  taxonomy of web actions to facilitate identifying common
                  failure patterns, and 2) create an extensible benchmark suite
                  to assess agents' performance on our taxonomized actions. This
                  benchmark suite consists of both individual tasks, such as
                  clicking a button, and end-to-end tasks, such as adding an
                  item to a cart, and is designed such that any failure of a
                  task can be attributed directly to a failure of a specific web
                  action. We evaluate two popular generalist web agents, one
                  text-based and one multimodal, and identify unique weaknesses
                  for each agent. Because WebSuite can disaggregate task
                  failures into specific action failures, this enables granular
                  identification of which UX flows an individual agent has
                  trouble with and immediately highlights promising avenues for
                  improvement. These findings highlight the need for more
                  focused benchmarking on where web agents go wrong to
                  effectively improve agents beyond their weaker performance
                  today.},
  urldate      = {2024-09-29}
}

@ARTICLE{Rein2023-cn,
  title        = {{GPQA}: A Graduate-Level Google-Proof {Q\&A} Benchmark},
  author       = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and
                  Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien
                  and Michael, Julian and Bowman, Samuel R},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-11-20},
  eprintclass  = {cs.AI},
  abstract     = {We present GPQA, a challenging dataset of 448 multiple-choice
                  questions written by domain experts in biology, physics, and
                  chemistry. We ensure that the questions are high-quality and
                  extremely difficult: experts who have or are pursuing PhDs in
                  the corresponding domains reach 65\% accuracy (74\% when
                  discounting clear mistakes the experts identified in
                  retrospect), while highly skilled non-expert validators only
                  reach 34\% accuracy, despite spending on average over 30
                  minutes with unrestricted access to the web (i.e., the
                  questions are "Google-proof"). The questions are also
                  difficult for state-of-the-art AI systems, with our strongest
                  GPT-4 based baseline achieving 39\% accuracy. If we are to use
                  future AI systems to help us answer very hard questions, for
                  example, when developing new scientific knowledge, we need to
                  develop scalable oversight methods that enable humans to
                  supervise their outputs, which may be difficult even if the
                  supervisors are themselves skilled and knowledgeable. The
                  difficulty of GPQA both for skilled non-experts and frontier
                  AI systems should enable realistic scalable oversight
                  experiments, which we hope can help devise ways for human
                  experts to reliably get truthful information from AI systems
                  that surpass human capabilities.}
}

@ARTICLE{Chen2021-fk,
  title        = {Evaluating large language models trained on code},
  author       = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming
                  and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and
                  Edwards, Harri and Burda, Yuri and Joseph, Nicholas and
                  Brockman, Greg and Ray, Alex and Puri, Raul and Krueger,
                  Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry,
                  Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott
                  and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and
                  Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and
                  Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave
                  and Plappert, Matthias and Chantzis, Fotios and Barnes,
                  Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and
                  Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie
                  and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and
                  Saunders, William and Hesse, Christopher and Carr, Andrew N
                  and Leike, Jan and Achiam, Josh and Misra, Vedant and
                  Morikawa, Evan and Radford, Alec and Knight, Matthew and
                  Brundage, Miles and Murati, Mira and Mayer, Katie and
                  Welinder, Peter and McGrew, Bob and Amodei, Dario and
                  McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  journaltitle = {arXiv [cs.LG]},
  date         = {2021-07-07},
  eprintclass  = {cs.LG},
  abstract     = {We introduce Codex, a GPT language model fine-tuned on
                  publicly available code from GitHub, and study its Python
                  code-writing capabilities. A distinct production version of
                  Codex powers GitHub Copilot. On HumanEval, a new evaluation
                  set we release to measure functional correctness for
                  synthesizing programs from docstrings, our model solves 28.8\%
                  of the problems, while GPT-3 solves 0\% and GPT-J solves
                  11.4\%. Furthermore, we find that repeated sampling from the
                  model is a surprisingly effective strategy for producing
                  working solutions to difficult prompts. Using this method, we
                  solve 70.2\% of our problems with 100 samples per problem.
                  Careful investigation of our model reveals its limitations,
                  including difficulty with docstrings describing long chains of
                  operations and with binding operations to variables. Finally,
                  we discuss the potential broader impacts of deploying powerful
                  code generation technologies, covering safety, security, and
                  economics.},
  urldate      = {2024-09-29}
}

@ARTICLE{Wang2023-jw,
  title        = {{MINT}: Evaluating {LLMs} in multi-turn interaction with tools
                  and language feedback},
  author       = {Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen,
                  Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-09-19},
  eprintclass  = {cs.CL},
  abstract     = {To solve complex tasks, large language models (LLMs) often
                  require multiple rounds of interactions with the user,
                  sometimes assisted by external tools. However, current
                  evaluation protocols often emphasize benchmark performance
                  with single-turn exchanges, neglecting the nuanced
                  interactions among the user, LLMs, and external tools, while
                  also underestimating the importance of natural language
                  feedback from users. These oversights contribute to
                  discrepancies between research benchmark evaluations and
                  real-world use cases. We introduce MINT, a benchmark that
                  evaluates LLMs' ability to solve tasks with multi-turn
                  interactions by (1) using tools and (2) leveraging natural
                  language feedback. To ensure reproducibility, we provide an
                  evaluation framework where LLMs can access tools by executing
                  Python code and receive users' natural language feedback
                  simulated by GPT-4. We repurpose a diverse set of established
                  evaluation datasets focusing on reasoning, coding, and
                  decision-making and carefully curate them into a compact
                  subset for efficient evaluation. Our analysis of 20 open- and
                  closed-source LLMs offers intriguing findings. (a) LLMs
                  generally benefit from tools and language feedback, with
                  performance gains (absolute, same below) of 1-8\% for each
                  turn of tool use and 2-17\% with natural language feedback.
                  (b) Better single-turn performance does not guarantee better
                  multi-turn performance. (c) Surprisingly, on the LLMs
                  evaluated, supervised instruction-finetuning (SIFT) and
                  reinforcement learning from human feedback (RLHF) generally
                  hurt multi-turn capabilities. We expect MINT can help measure
                  progress and incentivize research in improving LLMs'
                  capabilities in multi-turn interactions, especially for
                  open-source communities where multi-turn human evaluation can
                  be less accessible compared to commercial LLMs with a larger
                  user base.},
  urldate      = {2024-09-29}
}

@ARTICLE{Kapoor2024-dr,
  title        = {{AI} Agents That Matter},
  author       = {Kapoor, Sayash and Stroebl, Benedikt and Siegel, Zachary S and
                  Nadgir, Nitya and Narayanan, Arvind},
  journaltitle = {arXiv [cs.LG]},
  date         = {2024-07-01},
  eprintclass  = {cs.LG},
  abstract     = {AI agents are an exciting new research direction, and agent
                  development is driven by benchmarks. Our analysis of current
                  agent benchmarks and evaluation practices reveals several
                  shortcomings that hinder their usefulness in real-world
                  applications. First, there is a narrow focus on accuracy
                  without attention to other metrics. As a result, SOTA agents
                  are needlessly complex and costly, and the community has
                  reached mistaken conclusions about the sources of accuracy
                  gains. Our focus on cost in addition to accuracy motivates the
                  new goal of jointly optimizing the two metrics. We design and
                  implement one such optimization, showing its potential to
                  greatly reduce cost while maintaining accuracy. Second, the
                  benchmarking needs of model and downstream developers have
                  been conflated, making it hard to identify which agent would
                  be best suited for a particular application. Third, many agent
                  benchmarks have inadequate holdout sets, and sometimes none at
                  all. This has led to agents that are fragile because they take
                  shortcuts and overfit to the benchmark in various ways. We
                  prescribe a principled framework for avoiding overfitting.
                  Finally, there is a lack of standardization in evaluation
                  practices, leading to a pervasive lack of reproducibility. We
                  hope that the steps we introduce for addressing these
                  shortcomings will spur the development of agents that are
                  useful in the real world and not just accurate on benchmarks.},
  urldate      = {2024-09-28}
}

@ARTICLE{Wallace2024-qu,
  title        = {The instruction hierarchy: Training {LLMs} to prioritize
                  privileged instructions},
  author       = {Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian
                  and Heidecke, Johannes and Beutel, Alex},
  journaltitle = {arXiv [cs.CR]},
  date         = {2024-04-19},
  eprintclass  = {cs.CR},
  abstract     = {Today's LLMs are susceptible to prompt injections, jailbreaks,
                  and other attacks that allow adversaries to overwrite a
                  model's original instructions with their own malicious
                  prompts. In this work, we argue that one of the primary
                  vulnerabilities underlying these attacks is that LLMs often
                  consider system prompts (e.g., text from an application
                  developer) to be the same priority as text from untrusted
                  users and third parties. To address this, we propose an
                  instruction hierarchy that explicitly defines how models
                  should behave when instructions of different priorities
                  conflict. We then propose a data generation method to
                  demonstrate this hierarchical instruction following behavior,
                  which teaches LLMs to selectively ignore lower-privileged
                  instructions. We apply this method to GPT-3.5, showing that it
                  drastically increases robustness -- even for attack types not
                  seen during training -- while imposing minimal degradations on
                  standard capabilities.},
  urldate      = {2024-09-30}
}

@ARTICLE{Mu2023-hr,
  title        = {Can {LLMs} Follow Simple Rules?},
  author       = {Mu, Norman and Chen, Sarah and Wang, Zifan and Chen, Sizhe and
                  Karamardian, David and Aljeraisy, Lulwa and Alomair, Basel and
                  Hendrycks, Dan and Wagner, David},
  journaltitle = {arXiv [cs.AI]},
  date         = {2023-11-06},
  eprintclass  = {cs.AI},
  abstract     = {As Large Language Models (LLMs) are deployed with increasing
                  real-world responsibilities, it is important to be able to
                  specify and constrain the behavior of these systems in a
                  reliable manner. Model developers may wish to set explicit
                  rules for the model, such as "do not generate abusive
                  content", but these may be circumvented by jailbreaking
                  techniques. Existing evaluations of adversarial attacks and
                  defenses on LLMs generally require either expensive manual
                  review or unreliable heuristic checks. To address this issue,
                  we propose Rule-following Language Evaluation Scenarios
                  (RuLES), a programmatic framework for measuring rule-following
                  ability in LLMs. RuLES consists of 14 simple text scenarios in
                  which the model is instructed to obey various rules while
                  interacting with the user. Each scenario has a programmatic
                  evaluation function to determine whether the model has broken
                  any rules in a conversation. Our evaluations of proprietary
                  and open models show that almost all current models struggle
                  to follow scenario rules, even on straightforward test cases.
                  We also demonstrate that simple optimization attacks suffice
                  to significantly increase failure rates on test cases. We
                  conclude by exploring two potential avenues for improvement:
                  test-time steering and supervised fine-tuning.},
  urldate      = {2024-09-30}
}

@INPROCEEDINGS{Ansel2024-iu,
  title      = {{PyTorch} 2: Faster machine learning through dynamic python
                bytecode transformation and graph compilation},
  author     = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein,
                Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin
                and Bell, Peter and Berard, David and Burovski, Evgeni and
                Chauhan, Geeta and Chourdia, Anjali and Constable, Will and
                Desmaison, Alban and DeVito, Zachary and Ellison, Elias and
                Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh,
                Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch,
                Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo
                and Liang, Jason and Lu, Yinghai and Luk, C K and Maher, Bert
                and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and
                Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and
                Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu
                and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang,
                Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory
                and Wu, Peng and Chintala, Soumith},
  booktitle  = {Proceedings of the 29th ACM International Conference on
                Architectural Support for Programming Languages and Operating
                Systems, Volume 2},
  publisher  = {ACM},
  location   = {New York, NY, USA},
  eventtitle = {ASPLOS '24: 29th ACM International Conference on Architectural
                Support for Programming Languages and Operating Systems, Volume
                2},
  venue      = {La Jolla CA USA},
  date       = {2024-04-27},
  urldate    = {2024-10-07},
  language   = {en}
}

@ARTICLE{Shoeybi2019-si,
  title        = {Megatron-{LM}: Training multi-billion parameter language
                  models using model parallelism},
  author       = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and
                  LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journaltitle = {arXiv [cs.CL]},
  date         = {2019-09-17},
  eprintclass  = {cs.CL},
  abstract     = {Recent work in language modeling demonstrates that training
                  large transformer models advances the state of the art in
                  Natural Language Processing applications. However, very large
                  models can be quite difficult to train due to memory
                  constraints. In this work, we present our techniques for
                  training very large transformer models and implement a simple,
                  efficient intra-layer model parallel approach that enables
                  training transformer models with billions of parameters. Our
                  approach does not require a new compiler or library changes,
                  is orthogonal and complimentary to pipeline model parallelism,
                  and can be fully implemented with the insertion of a few
                  communication operations in native PyTorch. We illustrate this
                  approach by converging transformer based models up to 8.3
                  billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs
                  across the entire application with 76\% scaling efficiency
                  when compared to a strong single GPU baseline that sustains 39
                  TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that
                  large language models can further advance the state of the art
                  (SOTA), we train an 8.3 billion parameter transformer language
                  model similar to GPT-2 and a 3.9 billion parameter model
                  similar to BERT. We show that careful attention to the
                  placement of layer normalization in BERT-like models is
                  critical to achieving increased performance as the model size
                  grows. Using the GPT-2 model we achieve SOTA results on the
                  WikiText103 (10.8 compared to SOTA perplexity of 15.8) and
                  LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets.
                  Our BERT model achieves SOTA results on the RACE dataset
                  (90.9\% compared to SOTA accuracy of 89.4\%).},
  urldate      = {2024-10-21}
}

@ARTICLE{Dao2022-ji,
  title        = {{FlashAttention}: Fast and memory-efficient exact attention
                  with {IO}-awareness},
  author       = {Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri
                  and Ré, Christopher},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-05-27},
  eprintclass  = {cs.LG},
  abstract     = {Transformers are slow and memory-hungry on long sequences,
                  since the time and memory complexity of self-attention are
                  quadratic in sequence length. Approximate attention methods
                  have attempted to address this problem by trading off model
                  quality to reduce the compute complexity, but often do not
                  achieve wall-clock speedup. We argue that a missing principle
                  is making attention algorithms IO-aware -- accounting for
                  reads and writes between levels of GPU memory. We propose
                  FlashAttention, an IO-aware exact attention algorithm that
                  uses tiling to reduce the number of memory reads/writes
                  between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.
                  We analyze the IO complexity of FlashAttention, showing that
                  it requires fewer HBM accesses than standard attention, and is
                  optimal for a range of SRAM sizes. We also extend
                  FlashAttention to block-sparse attention, yielding an
                  approximate attention algorithm that is faster than any
                  existing approximate attention method. FlashAttention trains
                  Transformers faster than existing baselines: 15\% end-to-end
                  wall-clock speedup on BERT-large (seq. length 512) compared to
                  the MLPerf 1.1 training speed record, 3$\times$ speedup on
                  GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range
                  arena (seq. length 1K-4K). FlashAttention and block-sparse
                  FlashAttention enable longer context in Transformers, yielding
                  higher quality models (0.7 better perplexity on GPT-2 and 6.4
                  points of lift on long-document classification) and entirely
                  new capabilities: the first Transformers to achieve
                  better-than-chance performance on the Path-X challenge (seq.
                  length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K,
                  63.1\% accuracy).},
  urldate      = {2024-10-19}
}

@ARTICLE{Zheng2022-wf,
  title        = {Alpa: Automating inter- and intra-operator parallelism for
                  distributed deep learning},
  author       = {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang,
                  Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida
                  and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and
                  Gonzalez, Joseph E and Stoica, Ion},
  journaltitle = {arXiv [cs.LG]},
  date         = {2022-01-28},
  eprintclass  = {cs.LG},
  abstract     = {Alpa automates model-parallel training of large deep learning
                  (DL) models by generating execution plans that unify data,
                  operator, and pipeline parallelism. Existing model-parallel
                  training systems either require users to manually create a
                  parallelization plan or automatically generate one from a
                  limited space of model parallelism configurations. They do not
                  suffice to scale out complex DL models on distributed compute
                  devices. Alpa distributes the training of large DL models by
                  viewing parallelisms as two hierarchical levels:
                  inter-operator and intra-operator parallelisms. Based on it,
                  Alpa constructs a new hierarchical space for massive
                  model-parallel execution plans. Alpa designs a number of
                  compilation passes to automatically derive efficient parallel
                  execution plans at each parallelism level. Alpa implements an
                  efficient runtime to orchestrate the two-level parallel
                  execution on distributed compute devices. Our evaluation shows
                  Alpa generates parallelization plans that match or outperform
                  hand-tuned model-parallel training systems even on models they
                  are designed for. Unlike specialized systems, Alpa also
                  generalizes to models with heterogeneous architectures and
                  models without manually-designed plans. Alpa's source code is
                  publicly available at https://github.com/alpa-projects/alpa},
  urldate      = {2024-10-21}
}

@ARTICLE{Liu2023-em,
  title        = {Ring Attention with Blockwise Transformers for near-infinite
                  context},
  author       = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journaltitle = {arXiv [cs.CL]},
  date         = {2023-10-03},
  eprintclass  = {cs.CL},
  abstract     = {Transformers have emerged as the architecture of choice for
                  many state-of-the-art AI models, showcasing exceptional
                  performance across a wide range of AI applications. However,
                  the memory demands imposed by Transformers limit their ability
                  to handle long sequences, thereby posing challenges in
                  utilizing videos, actions, and other long-form sequences and
                  modalities in complex environments. We present a novel
                  approach, Ring Attention with Blockwise Transformers (Ring
                  Attention), which leverages blockwise computation of
                  self-attention and feedforward to distribute long sequences
                  across multiple devices while fully overlapping the
                  communication of key-value blocks with the computation of
                  blockwise attention. Our approach enables training and
                  inference of sequences that are up to device count times
                  longer than those achievable by prior memory-efficient
                  Transformers, without resorting to approximations or incurring
                  additional communication and computation overheads. Extensive
                  experiments on language modeling and reinforcement learning
                  tasks demonstrate the effectiveness of our approach in
                  allowing millions of tokens context size and improving
                  performance.},
  urldate      = {2024-10-19}
}

@ARTICLE{Jouppi2023-un,
  title        = {{TPU} {v4}: An optically reconfigurable supercomputer for
                  machine learning with hardware support for embeddings},
  author       = {Jouppi, Norman P and Kurian, George and Li, Sheng and Ma,
                  Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant
                  and Subramanian, Suvinay and Swing, Andy and Towles, Brian and
                  Young, Cliff and Zhou, Xiang and Zhou, Zongwei and Patterson,
                  David},
  journaltitle = {arXiv [cs.AR]},
  date         = {2023-04-03},
  eprintclass  = {cs.AR},
  abstract     = {In response to innovations in machine learning (ML) models,
                  production workloads changed radically and rapidly. TPU v4 is
                  the fifth Google domain specific architecture (DSA) and its
                  third supercomputer for such ML models. Optical circuit
                  switches (OCSes) dynamically reconfigure its interconnect
                  topology to improve scale, availability, utilization,
                  modularity, deployment, security, power, and performance;
                  users can pick a twisted 3D torus topology if desired. Much
                  cheaper, lower power, and faster than Infiniband, OCSes and
                  underlying optical components are <5\% of system cost and <3\%
                  of system power. Each TPU v4 includes SparseCores, dataflow
                  processors that accelerate models that rely on embeddings by
                  5x-7x yet use only 5\% of die area and power. Deployed since
                  2020, TPU v4 outperforms TPU v3 by 2.1x and improves
                  performance/Watt by 2.7x. The TPU v4 supercomputer is 4x
                  larger at 4096 chips and thus ~10x faster overall, which along
                  with OCS flexibility helps large language models. For similar
                  sized systems, it is ~4.3x-4.5x faster than the Graphcore IPU
                  Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than
                  the Nvidia A100. TPU v4s inside the energy-optimized warehouse
                  scale computers of Google Cloud use ~3x less energy and
                  produce ~20x less CO2e than contemporary DSAs in a typical
                  on-premise data center.},
  urldate      = {2024-10-07}
}

@ARTICLE{Penedo2024-mz,
  title        = {The {FineWeb} datasets: Decanting the web for the finest text
                  data at scale},
  author       = {Penedo, Guilherme and Kydlíček, Hynek and Allal, Loubna Ben
                  and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin
                  and Von Werra, Leandro and Wolf, Thomas},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-06-25},
  eprintclass  = {cs.CL},
  abstract     = {The performance of a large language model (LLM) depends
                  heavily on the quality and size of its pretraining dataset.
                  However, the pretraining datasets for state-of-the-art open
                  LLMs like Llama 3 and Mixtral are not publicly available and
                  very little is known about how they were created. In this
                  work, we introduce FineWeb, a 15-trillion token dataset
                  derived from 96 Common Crawl snapshots that produces
                  better-performing LLMs than other open pretraining datasets.
                  To advance the understanding of how best to curate
                  high-quality pretraining datasets, we carefully document and
                  ablate all of the design choices used in FineWeb, including
                  in-depth investigations of deduplication and filtering
                  strategies. In addition, we introduce FineWeb-Edu, a
                  1.3-trillion token collection of educational text filtered
                  from FineWeb. LLMs pretrained on FineWeb-Edu exhibit
                  dramatically better performance on knowledge- and
                  reasoning-intensive benchmarks like MMLU and ARC. Along with
                  our datasets, we publicly release our data curation codebase
                  and all of the models trained during our ablation experiments.},
  urldate      = {2024-10-12}
}

@ARTICLE{Soldaini2024-bj,
  title        = {Dolma: An open corpus of three trillion tokens for language
                  model pretraining research},
  author       = {Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and
                  Schwenk, Dustin and Atkinson, David and Authur, Russell and
                  Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar,
                  Yanai and Hofmann, Valentin and Jha, Ananya Harsh and Kumar,
                  Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and
                  Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and
                  Naik, Aakanksha and Nam, Crystal and Peters, Matthew E and
                  Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang
                  and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind
                  and Walsh, Pete and Zettlemoyer, Luke and Smith, Noah A and
                  Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and
                  Dodge, Jesse and Lo, Kyle},
  journaltitle = {arXiv [cs.CL]},
  date         = {2024-01-31},
  eprintclass  = {cs.CL},
  abstract     = {Information about pretraining corpora used to train the
                  current best-performing language models is seldom discussed:
                  commercial models rarely detail their data, and even open
                  models are often released without accompanying training data
                  or recipes to reproduce them. As a result, it is challenging
                  to conduct and advance scientific research on language
                  modeling, such as understanding how training data impacts
                  model capabilities and limitations. To facilitate scientific
                  research on language model pretraining, we curate and release
                  Dolma, a three-trillion-token English corpus, built from a
                  diverse mixture of web content, scientific papers, code,
                  public-domain books, social media, and encyclopedic materials.
                  We extensively document Dolma, including its design
                  principles, details about its construction, and a summary of
                  its contents. We present analyses and experimental results on
                  intermediate states of Dolma to share what we have learned
                  about important data curation practices. Finally, we
                  open-source our data curation toolkit to enable reproduction
                  of our work as well as support further research in large-scale
                  data curation.},
  urldate      = {2024-10-12}
}
